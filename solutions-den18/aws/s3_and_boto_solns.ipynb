{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: S3 on AWS\n",
    "\n",
    "S3 is the storage system on AWS. Here, you will practice interacting with it via the Amazon GUI\n",
    "and with the Python library `boto3`. You should know how to read and write files to S3 using a\n",
    "python script at the end of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Log into your [Amazon account](http://aws.amazon.com/console/), and create an S3 bucket using the GUI.\n",
    "   **The bucket name must be globally unique (not used on any AWS account).**\n",
    "\n",
    "   [Rules for S3 Bucket Names](http://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html):\n",
    "   * Bucket names must be at least 3 and no more than 63 characters long.\n",
    "   * Bucket names can contain lowercase letters, numbers, and hyphens.\n",
    "   * Periods are allowed but can cause problems. Avoid using periods.\n",
    "   * Bucket names cannot start or end with a hyphen or period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Upload (using the GUI) `data/cancer.csv` to your bucket, and note the link to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this would be the object URL\n",
    "#example: https://g90-demo-bucket-nick.s3-us-west-2.amazonaws.com/cancer.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use `read_csv()` in `pandas` to read in the file from S3 (you can treat the S3 URL as a file path). Include the `chunksize` argument in `read_csv`\n",
    "   to read in a subset of the file. In this case, with 301 rows, you would not need to subset your data.\n",
    "   For larger datasets, this would become handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the boto resource and client for downloading/uploading files\n",
    "s3_connection = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When your files and/or bucket are set to private, use this function for loading in data\n",
    "#Private buckets and files are recommended as a good general practice\n",
    "def load_csv_from_s3(bucketname, filename, n_rows=300):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        bucketname (str): Name of bucket that file is stored in\n",
    "        filename (str): Name of csv within bucket (ex: \"cool_data.csv\")\n",
    "        \n",
    "    Output:\n",
    "        pandas dataframe of csv (assuming no read_csv arguments are needed)\n",
    "    \"\"\"\n",
    "    \n",
    "    boto_object = s3_client.get_object(Bucket=bucketname, Key=filename)\n",
    "    return pd.read_csv(boto_object['Body'], nrows=n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'g90-demo-bucket-nick' #name of bucket (change to your personal bucket name)\n",
    "csv_name = 'cancer.csv' #name of file\n",
    "\n",
    "df = load_csv_from_s3(bucket, csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ONLY USE pd.read_csv IF YOUR BUCKET AND FILE ARE PUBLIC (this is not recommended)\n",
    "df = pd.read_csv(\"\"\"This is where you'd put the object URL from question 2\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the rates of cancer for each row, and make a histogram of the rates. Save the histogram as a `.png`\n",
    "   file using `savefig` in matplotlib. Save a `.csv` file of the rates you use for the histogram as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cancer_rate\"] = df[\"cancer\"] / df[\"population\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cancer_rates(x, save_figure=False, plot_save_path=None):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x_ticks = np.linspace(0, 0.01, 10)\n",
    "    x_tick_labels = [str(np.around((tick * 100), 2)) + \"%\" for tick in x_ticks]\n",
    "    \n",
    "    ax.hist(x, bins=50)\n",
    "    ax.set_xlabel(\"Cancer Rates\")\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_tick_labels, rotation=45)\n",
    "    \n",
    "    ax.set_ylabel(\"Counts\")\n",
    "    ax.set_title(\"Frequency of Cancer Rates\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_figure:\n",
    "        plt.savefig(plot_save_path, dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_cancer_rates(df[\"cancer_rate\"].values, save_figure=True, plot_save_path=\"cancer_rates.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df_filename = \"cancer_rates.csv\"\n",
    "df[\"cancer_rate\"].to_csv(cancer_df_filename, header=[\"cancer_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write a script using `boto3` to upload the histogram `.png` and the rates `.csv` to the bucket you have created.\n",
    "   Confirm you have uploaded the files by checking the GUI console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(cancer_df_filename, bucket, cancer_df_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: EC2 on AWS\n",
    "\n",
    "EC2 is a remote virtual machine that runs programs much like your local machine. Here you will learn how to\n",
    "run tasks on an EC2 machine. Most EC2 instances come without a lot of the packages you need. Here, we will use\n",
    "an instance that has most of the data science packages installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an EC2 instance. Search for a Machine Image (Community AMI) that has `anaconda3` and `Ubuntu`. **Optional - you could choose only Ubuntu as the AMI and then build your instance with anaconda3 from the ground up.  See directions at the end of this guide.** Choose `t2.micro` for the instance type. Give the instance an IAM role that allows it full access to S3. Choose an *all-lowercase* name for the instance and add a `Name` tag (Key=`Name`, Value=`examplename`). Careful: Do not replace `Name` in the key field. Set the value instead by replacing `examplename`.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When creating the IAM role for your EC2 instance:\n",
    "        \n",
    "        1) Make sure to specify that it will be used by EC2 instances\n",
    "        \n",
    "        2) For the permission policy, choose \"AmazonS3FullAccess\"\n",
    "        \n",
    "When creating your EC2 instance:\n",
    "\n",
    "        1) Make sure to select your new IAM role under \"Configure Instance Details\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Log into the instance you have launched using `ssh`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssh -i path/to/key.pem ubuntu@ec2_url_here\n",
    "\n",
    "\"\"\"\n",
    "Note:\n",
    "    Replace \"ubuntu\" with ec2-user if running an Amazon Linux AMI rather than an Ubuntu AMI\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Update `apt` sources and perform routine updates:\n",
    "\n",
    "```\n",
    "sudo apt update\n",
    "sudo apt upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Modify the script you have written to process `cancer.csv` in `Part 1`. Instead of writing the results to\n",
    "   the same S3 bucket as where `cancer.csv` is, change the script to write to a new bucket.  \n",
    "\n",
    "   You will have to modify the script in another way, because EC2 linux servers don't have the same visual resources as your laptop.  Therefore, you'll need to change how you import `matplotlib.`  Modify the import in your script:\n",
    "   ```python\n",
    "   import matplotlib\n",
    "   matplotlib.use(\"Agg\")\n",
    "   import matplotlib.pyplot as plt\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following code would be ran on the EC2 instance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s3_connection = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "def load_csv_from_s3(bucketname, filename, n_rows=300):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        bucketname (str): Name of bucket that file is stored in\n",
    "        filename (str): Name of csv within bucket (ex: \"cool_data.csv\")\n",
    "        \n",
    "    Output:\n",
    "        pandas dataframe of csv (assuming no read_csv arguments are needed)\n",
    "    \"\"\"\n",
    "    \n",
    "    boto_object = s3_client.get_object(Bucket=bucketname, Key=filename)\n",
    "    return pd.read_csv(boto_object['Body'], nrows=n_rows)\n",
    "\n",
    "def plot_cancer_rates(x, save_figure=False, plot_save_path=None):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x_ticks = np.linspace(0, 0.01, 10)\n",
    "    x_tick_labels = [str(np.around((tick * 100), 2)) + \"%\" for tick in x_ticks]\n",
    "    \n",
    "    ax.hist(x, bins=50)\n",
    "    ax.set_xlabel(\"Cancer Rates\")\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_xticklabels(x_tick_labels, rotation=45)\n",
    "    \n",
    "    ax.set_ylabel(\"Counts\")\n",
    "    ax.set_title(\"Frequency of Cancer Rates\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_figure:\n",
    "        plt.savefig(plot_save_path, dpi=500)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    new_bucket_name = 'aws-denver-branch-bucket'\n",
    "    csv_name = 'cancer.csv'\n",
    "    cancer_df_filename = \"cancer_rates.csv\"\n",
    "    plot_filename = \"cancer_rates.png\"\n",
    "    \n",
    "    df = load_csv_from_s3()\n",
    "    df[\"cancer_rate\"] = df[\"cancer\"] / df[\"population\"]\n",
    "    plot_cancer_rates(df[\"cancer_rate\"].values, save_figure=True, plot_save_path=plot_filename)\n",
    "    \n",
    "    df[\"cancer_rate\"].to_csv(cancer_df_filename, header=[\"cancer_rate\"])\n",
    "    \n",
    "    \n",
    "    s3_client.create_bucket(Bucket=new_bucket_name)\n",
    "    \n",
    "    s3_client.upload_file(cancer_df_filename, new_bucket_name, cancer_df_filename)\n",
    "    s3_client.upload_file(plot_filename, new_bucket_name, plot_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use `scp` or `git` to copy the script onto the EC2 instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you need help installing/configuring git on your ec2 instance\n",
    "#try this link: https://cloudaffaire.com/how-to-install-git-in-aws-ec2-instance/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Run the script on the EC2 instance and check S3 to make sure the results are transferred to a new bucket. In practice, you will be testing the script locally with a smaller subset of the data, and run the script on the whole set on EC2. If your task requires more processing power, you have the option to run it on a more powerful EC2 instance with more RAM and more cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional (if you didn't use an anaconda3 AMI)**\n",
    "\n",
    "Install Anaconda\n",
    "\n",
    "```\n",
    "# Download Anaconda3\n",
    "wget -S -T 10 -t 5 https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh -O $HOME/anaconda.sh\n",
    "\n",
    "# Install Anaconda\n",
    "bash anaconda.sh\n",
    "\n",
    "# when prompted for an installation path, \n",
    "# press \"enter\" to accpet the default\n",
    "\n",
    "\n",
    "# when prompted to \"prepend the install location \n",
    "# to your PATH\", type 'yes'\n",
    "\n",
    "# once installation is finished, you still have\n",
    "# to execute the commands in ~/.bashrc\n",
    "source ~/.bashrc\n",
    "\n",
    "conda install boto3\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
