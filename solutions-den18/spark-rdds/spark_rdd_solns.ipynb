{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps    # for the pyspark suite\n",
    "import warnings         # for displaying warning\n",
    "import os               # for environ variables in Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: RDD and Spark Basics\n",
    "\n",
    "Here we will get familiar with the basics of Spark via the Spark Python API,\n",
    "`pyspark` module in python. For now, we will be just working with a single node that will\n",
    "parallelize processes across all of our cores (rather than distributing them\n",
    "across worker nodes).\n",
    "\n",
    "1\\. Initiate a `SparkSession`. A `SparkSession` embeds both a `SparkContext` and a `SQLContext` to use RDD-based and DataFrame-based functionalities of Spark. Specify your `SparkSession` as follows.\n",
    "\n",
    "```python\n",
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"df lecture\") \\\n",
    "        .getOrCreate()\n",
    "```\n",
    "\n",
    "Create a variable `sc` using the following line. It will let you use `sc` as a `sparkContext` for compatibility with pre-2.0 RDD-based spark commands.\n",
    "\n",
    "```\n",
    "sc = spark.sparkContext\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "\n",
    "spark = ps.sql.SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"df lecture\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Spark operates in **[Resilient Distributed Datasets](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds) (RDDs). An RDD is\n",
    "a collection of data partitioned across machines**. RDDs allow the processing\n",
    "of data to be parallelized due to the partitions. RDDs can be created from\n",
    "a SparkContext in two ways: loading an external dataset, or by parallelizing\n",
    "an existing collection of objects in your currently running program (in our\n",
    "Python programs, this is often times a list).\n",
    "\n",
    "* Create an RDD from a Python list.\n",
    "\n",
    "```python\n",
    "lst_rdd = sc.parallelize([1, 2, 3])\n",
    "```\n",
    "\n",
    "* Read an RDD in from a text file. **By default, the RDD will treat each line\n",
    "as an item and read it in as string.**\n",
    "\n",
    "```python\n",
    "file_rdd = sc.textFile('data/cookie_data.txt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rdd = sc.textFile('data/cookie_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Now that we have an RDD, we need to see what is inside. RDDs by default will\n",
    "  load data into partitions across the machines on your cluster. This means that\n",
    "  you can quickly check out the first few entries of a potentially enormous RDD\n",
    "  without accessing all of the partitions and loading all of the data into memory.\n",
    "\n",
    "```python\n",
    "file_rdd.first() # Returns the first entry in the RDD\n",
    "file_rdd.take(2) # Returns the first two entries in the RDD as a list\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"Jane\": \"2\"}',\n",
       " '{\"Jane\": \"1\"}',\n",
       " '{\"Pete\": \"20\"}',\n",
       " '{\"Tyler\": \"3\"}',\n",
       " '{\"Duncan\": \"4\"}']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. To retrieve all the items in your RDD, every partition in the RDD has to be\n",
    "  accessed, and this could take a long time. In general, before you execute\n",
    "  commands (like the following) to retrieve all the items in your RDD, you\n",
    "  should be aware of how many entries you are pulling. Keep in mind that to\n",
    "  execute the `.collect()` method on the RDD object (like we do below), your entire\n",
    "  dataset must fit in memory in your driver program (we in general don't want\n",
    "  to call `.collect()` on very large datasets).\n",
    "\n",
    "  The standard workflow when working with RDDs is to perform all the big data\n",
    "  operations/transformations **before** you pool/retrieve the results. If the\n",
    "  results can't be collected onto your driver program, it's common to write\n",
    "  data out to a distributed storage system, like HDFS or S3.\n",
    "\n",
    "  With that said, we can retrieve all the items from our RDD as follows:\n",
    "\n",
    "```python\n",
    "file_rdd.collect()\n",
    "lst_rdd.collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"Jane\": \"2\"}',\n",
       " '{\"Jane\": \"1\"}',\n",
       " '{\"Pete\": \"20\"}',\n",
       " '{\"Tyler\": \"3\"}',\n",
       " '{\"Duncan\": \"4\"}',\n",
       " '{\"Yuki\": \"5\"}',\n",
       " '{\"Duncan\": \"6\"}',\n",
       " '{\"Duncan\": \"4\"}',\n",
       " '{\"Duncan\": \"5\"}']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Intro to Functional Programming\n",
    "\n",
    "Spark operations fit within the [functional programming paradigm](https://en.wikipedia.org/wiki/Functional_programming).\n",
    "In terms of our RDD objects, this means that our RDD objects are immutable and that\n",
    "anytime we apply a **transformation** to an RDD (such as `.map()`, `.reduceByKey()`,\n",
    "or `.filter()`) it returns another RDD.\n",
    "\n",
    "Transformations in Spark are lazy, this means that performing a transformation does\n",
    "not cause computations to be performed. Instead, an RDD remembers the chain of\n",
    "transformations that you define and computes them all only when and action requires\n",
    "a result to be returned.\n",
    "\n",
    "**Spark notes**:\n",
    "\n",
    "  * A lot of Spark's functionalities assume the items in an RDD to be tuples\n",
    "  of `(key, value)` pairs, so often times it can be useful to structure your\n",
    "  RDDs this way.\n",
    "  * Beware of [lazy evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation), where transformations\n",
    "  on the RDD are not executed until an **action** is executed on the RDD\n",
    "  to retrieve items from it (such as `.collect()`, `.first()`, `.take()`, or\n",
    "  `.count()`). So if you are doing a lot transformations in a row, it can\n",
    "  be helpful to call `.first()` in between to ensure your transformations are\n",
    "  running properly.\n",
    "  * If you are not sure what RDD transformations/actions there are, you can\n",
    "  check out the [docs](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD).\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1\\. Turn the items in `file_rdd` into `(key, value)` pairs using `.map()`. In order to do that, you'll find a template function `parse_json_first_key_pair` in the `spark_intro.py` file. Implement this function that takes a json formatted string (use `json.loads()`) and output the key,value pair you need. Test it with the string `u'{\"Jane\": \"2\"}'`, your function should return `(u'Jane', 2)`. **Remember to cast value as type** `int`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jane', 2), ('Jane', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spark_intro_solns import parse_json_first_key_pair\n",
    "\n",
    "# apply the map function\n",
    "pairs_rdd = file_rdd.map(parse_json_first_key_pair)\n",
    "\n",
    "# take 2 to check result\n",
    "pairs_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2\\. Now use `.filter()` to look for entries with more than `5` chocolate chip cookies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pete', 20), ('Yuki', 5)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a lambda function to apply filter (should return True/False)\n",
    "filtered_rdd = pairs_rdd.filter(lambda row: row[1] >= 5)\n",
    "\n",
    "# take 2 to check result\n",
    "filtered_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. For each name, return the entry with the max number of cookies.\n",
    "\n",
    "**Hint**:\n",
    "* Use `.reduceByKey()` instead of `.groupByKey()`. See why [here](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html).\n",
    "* You may get a warning saying that you should install `psutil`. You can with\n",
    "`pip install psutil`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we use reduceByKey we can rely on python's max function to compare tuples\n",
    "# with a matching value in the zeroth index by the first index.\n",
    "# the lambda given to reduceByKey() will be used to combine the values in the (key,value) pairs of your rdd\n",
    "reduced_rdd = pairs_rdd.reduceByKey(lambda a,b: max(a,b))\n",
    "\n",
    "# let's apply filter\n",
    "output_rdd = reduced_rdd.filter(lambda x: x[1] >= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Let's show the first results using `.sortBy()` and `.take()`. `.sortBy()` requires a lambda function that outputs the value/quantity on which we want to sort our rows. Because we currently have only one value, you will use **`lambda (k, v): v`** or **`lambda x: x[1]`** (they are equivalent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pete', 20), ('Duncan', 6), ('Yuki', 5)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_rdd.sortBy(lambda kvtuple: kvtuple[1], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Calculate the total revenue from people buying cookies (we're assuming that\n",
    "each cookie only costs $1).\n",
    "\n",
    "**Hint**:\n",
    "* `rdd.values()` returns another RDD of all the values.\n",
    "* Use `.reduce()` to return the sum of all the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduces automatically calls the lambda and fill in corresponding values into x and y.\n",
    "# The first time the lambda is called x and y are the first two values of the sequence.\n",
    "# Once the first sum has occurred, that first output value fills into x\n",
    "# and the next value in the sequence fills into y.\n",
    "pairs_rdd.values().reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Spark for Data Processing\n",
    "\n",
    "  We will now explore airline data. The data are stored on S3 so you will need your AWS access key and secret access key.\n",
    "\n",
    "### Side Note About Personal Credentials\n",
    "\n",
    "  It's good practice to keep personal credentials stored in environment variables set in\n",
    "  your bash profile so that you don't have to hard code their values into your solutions.\n",
    "  This is particularly important when the code that uses your keys is stored on GitHub\n",
    "  since you don't want to be sharing your access keys with the world. To do this make\n",
    "  add the lines below to your bash profile.\n",
    "\n",
    "\n",
    "```bash\n",
    "export AWS_ACCESS_KEY_ID=YOUR ACCESS KEY\n",
    "export AWS_SECRET_ACCESS_KEY=YOUR SECRET ACCESS KEY\n",
    "```\n",
    "\n",
    "  Keep in mind that if you ever have to change your keys you'll need to make sure that you\n",
    "  update your bash profile.\n",
    "\n",
    "  Now you're ready to load up and explore the data all while becoming more familiar with\n",
    "  Spark.\n",
    "\n",
    "  ### 3.1: Loading Data from an S3 bucket\n",
    "\n",
    "  1\\. Load the data from S3 as follows.\n",
    "\n",
    "```python\n",
    "\n",
    "link = 's3a://mortar-example-data/airline-data'\n",
    "airline_rdd = sc.textFile(link)\n",
    "```\n",
    "\n",
    "**Note**: If you ever encounter an issue using your AWS credentials, and if you want to skip that at this point to save time on the assignment, you'll find an extract of that dataset (100 lines) in `data/airline-data-extract.csv`. You can use this extract to develop your complete pipeline and solve your issue later on. Use `airline_rdd = sc.textFile(\"data/airline-data-extract.csv\")` to transform that extract into an RDD.\n",
    "\n",
    "---\n",
    "\n",
    "**NOTE**: In order to load data from s3, we need to launch our spark session with the `--packages` options for interfacing with aws and hadoop.  For Example:\n",
    "\n",
    "```bash\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --NotebookApp.open_browser=True --NotebookApp.ip='localhost' --NotebookApp.port=8888\"\n",
    "\n",
    "${SPARK_HOME}/bin/pyspark \\\n",
    "    --master local[4] \\\n",
    "    --executor-memory 1G \\\n",
    "    --driver-memory 1G \\\n",
    "    --conf spark.sql.warehouse.dir=\"file:///tmp/spark-warehouse\" \\\n",
    "    --packages com.databricks:spark-csv_2.11:1.5.0 \\\n",
    "    --packages com.amazonaws:aws-java-sdk-pom:1.10.34 \\\n",
    "    --packages org.apache.hadoop:hadoop-aws:2.7.3\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 's3a://mortar-example-data/airline-data'\n",
    "airline_rdd = sc.textFile(link)\n",
    "# use line below instead to run on a local extract\n",
    "#airline_rdd = sc.textFile(\"data/airline-data-extract.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Print the first 2 entries with `.take(2)` on `airline_rdd`. The first entry is the column names and starting with the second we have our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"YEAR\",\"MONTH\",\"UNIQUE_CARRIER\",\"ORIGIN_AIRPORT_ID\",\"DEST_AIRPORT_ID\",\"DEP_DELAY\",\"DEP_DELAY_NEW\",\"ARR_DELAY\",\"ARR_DELAY_NEW\",\"CANCELLED\",',\n",
       " '2012,4,\"AA\",12478,12892,-4.00,0.00,-21.00,0.00,0.00,']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Now run `.count()` on the RDD. **This will take a while**, as the data set is a few million rows and it all must be downloaded from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5113194"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Create a pipeline on a sub-sample dataset\n",
    "\n",
    "Now we can move on to looking at the data and transforming it. In this section we will operate only on a limited data set, develop a full pipeline and later on execute that on the full scale data.\n",
    "\n",
    "We want to identify airports with the worst / least delays. Consider the following about delays:\n",
    "\n",
    "* **2 types of delays:** Arrival delays, `ARR_DELAY`, and departure delays, `DEP_DELAY`.\n",
    "* All delays are in terms of **minutes**.\n",
    "* Arrival delays are associated with the destination airport, `DEST_AIRPORT_ID`.\n",
    "* Departure delays are associated with the origin airport, `ORIGIN_AIRPORT_ID`.\n",
    "\n",
    "\n",
    "1\\. As you just saw the `.count()` action takes a long time to run. More involved commands can take even longer. In order to not waste time when writing/testing your code, it's common practice to work with a sub-sample of your data until you have your code finalized/polished and ready to run on the full dataset. Use `.take(100)` to sample out the first 100 rows and assign it to a new RDD using `sc.parallelize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the line below to test your pipeline\n",
    "airline_small_rdd = sc.parallelize(airline_rdd.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"YEAR\",\"MONTH\",\"UNIQUE_CARRIER\",\"ORIGIN_AIRPORT_ID\",\"DEST_AIRPORT_ID\",\"DEP_DELAY\",\"DEP_DELAY_NEW\",\"ARR_DELAY\",\"ARR_DELAY_NEW\",\"CANCELLED\",',\n",
       " '2012,4,\"AA\",12478,12892,-4.00,0.00,-21.00,0.00,0.00,',\n",
       " '2012,4,\"AA\",12478,12892,-7.00,0.00,-65.00,0.00,0.00,',\n",
       " '2012,4,\"AA\",12478,12892,-6.00,0.00,-63.00,0.00,0.00,',\n",
       " '2012,4,\"AA\",12478,12892,-6.00,0.00,5.00,5.00,0.00,',\n",
       " '2012,4,\"AA\",12478,12892,-2.00,0.00,-39.00,0.00,0.00,',\n",
       " '2012,4,\"AA\",12478,12892,-6.00,0.00,-34.00,0.00,0.00,',\n",
       " '2012,4,\"AA\",12478,12892,-8.00,0.00,-16.00,0.00,0.00,',\n",
       " '2012,4,\"AA\",12478,12892,-7.00,0.00,-19.00,0.00,0.00,',\n",
       " '2012,4,\"AA\",12478,12892,-9.00,0.00,-2.00,0.00,0.00,']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_small_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Let's do some preprocessing and parsing. You may have noticed that those rows are in fact csv lines. We are going to parse those lines one by one and output a list of the values we can split from those lines.\n",
    "\n",
    "In order to do that, you'll find a template function `split_csvstring` in the `spark_intro.py` file. Implement this function that takes a string that contains a csv line, and output the list of values contained in the line. You can use a combination of the `csv` module function `csv.reader()` and the `StringIO` module.\n",
    "\n",
    "Test it with the string `'a,b,0.7,\"Oct 7, 2016\",42,'`, your function should return `['a', 'b', '0.7', 'Oct 7, 2016', '42', '']`\n",
    "\n",
    "Once your function works, use `.map()` to apply it to your RDD. Print the first 2 lines, with `take(2)`, to confirm you've cleaned the rows correctly. The first 2 lines should look like the following.\n",
    "\n",
    "```\n",
    "[['YEAR', 'MONTH', 'UNIQUE_CARRIER', 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID', 'DEP_DELAY', 'DEP_DELAY_NEW', 'ARR_DELAY', 'ARR_DELAY_NEW', 'CANCELLED', ''],\n",
    "['2012', '4', 'AA', '12478', '12892', '-4.00', '0.00', '-21.00', '0.00', '0.00', '']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['YEAR',\n",
       "  'MONTH',\n",
       "  'UNIQUE_CARRIER',\n",
       "  'ORIGIN_AIRPORT_ID',\n",
       "  'DEST_AIRPORT_ID',\n",
       "  'DEP_DELAY',\n",
       "  'DEP_DELAY_NEW',\n",
       "  'ARR_DELAY',\n",
       "  'ARR_DELAY_NEW',\n",
       "  'CANCELLED',\n",
       "  ''],\n",
       " ['2012',\n",
       "  '4',\n",
       "  'AA',\n",
       "  '12478',\n",
       "  '12892',\n",
       "  '-4.00',\n",
       "  '0.00',\n",
       "  '-21.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spark_intro_solns import split_csvstring\n",
    "\n",
    "# applying the split_csvstring() function to all rows in RDD\n",
    "clean_row_rdd = airline_small_rdd.map(split_csvstring)\n",
    "\n",
    "# take 2 for checking\n",
    "clean_row_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Use `filter()` with a `lambda` function to filter out the line containing the column names. Keep that line in a variable so that you can use in next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first line of RDD\n",
    "first_row = clean_row_rdd.first()\n",
    "\n",
    "# filter lines that are identical to the first line\n",
    "only_data_rdd = clean_row_rdd.filter(lambda row: row != first_row)\n",
    "\n",
    "# obtain column names (first row)\n",
    "column_names = first_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Write a function `make_row_dict()`, that takes a row (list of values) as an argument and returns a dictionary where the keys are column names and the values are the values for the column. Follow the specifications below to make your dictionary.\n",
    "\n",
    "The dictionary will only keep track of the following columns:\n",
    "\n",
    "`['DEST_AIRPORT_ID', 'ORIGIN_AIRPORT_ID', 'DEP_DELAY', 'ARR_DELAY']`\n",
    "* Cast the values for `DEP_DELAY` and `ARR_DELAY` as floats. These values\n",
    "correspond with delay lengths in minutes.\n",
    "* Subtract `DEP_DELAY` from `ARR_DELAY` to get the actual `ARR_DELAY`.\n",
    "* If a flight is `CANCELLED`, add 5 hours, 300 minutes, to `DEP_DELAY`.\n",
    "* There are missing values in `DEP_DELAY` and `ARR_DELAY` (i.e. `''`) and\n",
    " you would want to replace those with `0.0`.\n",
    "\n",
    "You'll find a template function `make_row_dict` in the `spark_intro.py` file with a `doctest` you can try to make it work, using `python -m doctest -v spark_intro.py`.\n",
    "\n",
    "Now use `.map()` with your function  `make_row_dict()` over your RDD to make a new RDD made of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ARR_DELAY': -17.0,\n",
       "  'DEP_DELAY': -4.0,\n",
       "  'DEST_AIRPORT_ID': '12892',\n",
       "  'ORIGIN_AIRPORT_ID': '12478'},\n",
       " {'ARR_DELAY': -58.0,\n",
       "  'DEP_DELAY': -7.0,\n",
       "  'DEST_AIRPORT_ID': '12892',\n",
       "  'ORIGIN_AIRPORT_ID': '12478'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spark_intro_solns import make_row_dict\n",
    "\n",
    "dict_keys = {'DEST_AIRPORT_ID', 'ORIGIN_AIRPORT_ID', 'DEP_DELAY', 'ARR_DELAY'}\n",
    "\n",
    "dict_rdd = only_data_rdd.map(lambda row: make_row_dict(row, column_names, dict_keys))\n",
    "dict_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on Solution**: what if we wanted to do that generically ? if we want to be able to apply any kind of transformation (type, values) on the columns ? find out in the `airline_pipeline.py` file, under the function `make_row_dict_generic`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Now we will use these dictionaries to create 2 RDDs, where the items are tuples. Remember, much of Spark's functionality assumes RDDs to be storing (key, value) tuples. You can `.map()` to create those RDDs using `lambda` functions applied to the RDD generated in 4.\n",
    "\n",
    "The first RDD will contain tuples `(DEST_AIRPORT_ID, ARR_DELAY)`. The other RDD will contain `(ORIGIN_AIRPORT_ID, DEP_DELAY)`. Run a `.first()` or `.take()` to confirm your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12892', -17.0), ('12892', -58.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrival_rdd = dict_rdd.map(lambda row: (row['DEST_AIRPORT_ID'], row['ARR_DELAY']))\n",
    "arrival_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12478', -4.0), ('12478', -7.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departure_rdd = dict_rdd.map(lambda row: (row['ORIGIN_AIRPORT_ID'], row['DEP_DELAY']))\n",
    "departure_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Using the two RDDs you just created, make 2 RDDs with the mean delay time for each origin airports and each destination airports. You will need to `.groupByKey()` and then take the mean of the delay times for each airport. Use `.mapValues()` to calculate the mean of each group's values.\n",
    "\n",
    "This is where having our RDDs be composed of `(key, value)` pairs is relevant.\n",
    "It allows us to use the `.groupByKey()` method on our RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a mean function to feed into groupByKey()\n",
    "def rdd_group_mean(value_group):\n",
    "    # transform groups produced by groupByKey into lists of values\n",
    "    values = list(value_group)\n",
    "    # so that we can compute their mean\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "average_departure_delay = departure_rdd.groupByKey().mapValues(lambda value_group: rdd_group_mean(value_group))\n",
    "average_arrival_delay = arrival_rdd.groupByKey().mapValues(lambda value_group: rdd_group_mean(value_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average departure delays: [('12478', -5.966666666666667), ('12892', 3.717948717948718)]\n",
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 351 ms\n",
      "average arrival delays: [('12892', -12.116666666666667), ('12478', -16.17948717948718)]\n",
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%time print(\"average departure delays: {}\".format( average_departure_delay.collect() ))\n",
    "\n",
    "%time print(\"average arrival delays: {}\".format( average_arrival_delay.collect() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There is a slightly more performant way of calculating the mean which uses\n",
    "`.aggregateByKey()` rather than `.groupByKey()`. This transformation models the combiner\n",
    "model that we saw in Hadoop. Unfortunately, the documentation for `.aggregateByKey()` is\n",
    "quite poor. Check out [this](http://stackoverflow.com/a/29930162) stack overflow post\n",
    "for a good description for how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this lambda function aggregates values within a partition\n",
    "# in the output tuple, component 1 is the sum of all values,\n",
    "# component 2 counts how many values have been summed\n",
    "within_partition_agg = lambda agg, value: (agg[0] + value, agg[1] + 1)\n",
    "\n",
    "# this lambda function aggregates two aggregates between partitions\n",
    "# in the output tuple, component 1 sums the values, component 2 sums the counts\n",
    "between_partition_agg = lambda agg1, agg2: (agg1[0] + agg2[0], agg1[1] + agg2[1])\n",
    "\n",
    "# run the aggregateByKey routine on departure_rdd\n",
    "departure_sum_count = departure_rdd.aggregateByKey((0, 0), within_partition_agg,\n",
    "                                                           between_partition_agg)\n",
    "\n",
    "# run the aggregateByKey routine on arrival_rdd\n",
    "arrival_sum_count = arrival_rdd.aggregateByKey((0, 0), within_partition_agg,\n",
    "                                                       between_partition_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average departure delays: [('12478', -5.966666666666667), ('12892', 3.717948717948718)]\n",
      "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
      "Wall time: 279 ms\n",
      "average arrival delays: [('12892', -12.116666666666667), ('12478', -16.17948717948718)]\n",
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 177 ms\n"
     ]
    }
   ],
   "source": [
    "%time print(\"average departure delays: {}\".format( departure_sum_count.mapValues(lambda _tuple: _tuple[0] / _tuple[1]).collect()) )\n",
    "\n",
    "%time print(\"average arrival delays: {}\".format( arrival_sum_count.mapValues(lambda _tuple: _tuple[0] / _tuple[1]).collect()) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Run `.cache()` on the RDDs you just made. Remember to set the name of the RDD using `.setName()` before running `.cache()` (e.g. `rdd.setName('airline_rdd').cache()`). Setting the name will allow you to identify the RDD in the Spark web UI (see extra credit).\n",
    "\n",
    "When you cache the RDDs, you make sure that computations which produced them don't\n",
    "need to be performed every time they are called upon. It is good practice to use `cache()`\n",
    "for RDDs that you are going to repeatedly use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_departure_delay = average_departure_delay.setName('avg_dep_delay').cache()\n",
    "average_arrival_delay = average_arrival_delay.setName('avg_arr_delay').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "8\\. Perform appropriate actions on your RDDs to answer the following questions:\n",
    "\n",
    "* Q1: What are the top 10 departing airports that have the lowest average delay?\n",
    "* Q2: What are the top 10 departing airports that have the highest average delay?\n",
    "* Q3: What are the top 10 arriving airports that have the lowest average delay?\n",
    "* Q4: What are the top 10 arriving airports that have the highest average delay?\n",
    "\n",
    "There are a couple of ways that you can do this. One is by using `sortBy()` and then\n",
    "`take(10)`. However, this is not the most efficient way. Why not?\n",
    "\n",
    "The other way, more efficient way to answer this question is with `takeOrdered()`.\n",
    "You'll have to be a little clever to get the highest delays. Check out the\n",
    "[docs](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeOrdered)\n",
    "for a hint.\n",
    "\n",
    "You'll need to run all the transformations that you tested on the smaller dataset\n",
    "on the full data set to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note on solution ** : in the following lines, we'll first test the sortBy function on the small local dataset (`airline_rdd` 100 rows) so that we make it right. Later on we will deploy that on the full rdd `airline_rdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12892', 3.717948717948718), ('12478', -5.966666666666667)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1: What are the top 10 departing airports that have the lowest average delay?\n",
    "average_departure_delay.sortBy(lambda kvtuple: kvtuple[1], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12478', -5.966666666666667), ('12892', 3.717948717948718)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2: What are the top 10 departing airports that have the highest average delay?\n",
    "average_departure_delay.sortBy(lambda kvtuple: kvtuple[1], ascending=True).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12892', -12.116666666666667), ('12478', -16.17948717948718)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3: What are the top 10 arriving airports that have the lowest average delay?\n",
    "average_arrival_delay.sortBy(lambda kvtuple: kvtuple[1], ascending=False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12478', -16.17948717948718), ('12892', -12.116666666666667)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4: What are the top 10 arriving airports that have the highest average delay?\n",
    "average_arrival_delay.sortBy(lambda kvtuple: kvtuple[1], ascending=True).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Assemble your pipeline and run it on the full scale dataset\n",
    "\n",
    "1\\. In `spark_intro.py` you'll find a function `transformation_pipeline` you will implement by embedding all the transformations we've done so far, starting from question 3.2.2 (creating a clean rdd) to question 3.2.8 (finding answers to Q1, Q2, Q3, Q4). The function should return the 4 result lists to questions Q1, Q2, Q3, Q4 in a tuple.\n",
    "\n",
    "Then, run this function from the jupyter notebook or from the main section in `spark_intro.py` to test it on your sub-sample rdd. You should obtain the same answers you had previously obtained on a step by step basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[24] at parallelize at PythonRDD.scala:175"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_small_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('12892', 3.717948717948718), ('12478', -5.966666666666667)], [('12478', -5.966666666666667), ('12892', 3.717948717948718)], [('12892', -12.116666666666667), ('12478', -16.17948717948718)], [('12478', -16.17948717948718), ('12892', -12.116666666666667)])\n"
     ]
    }
   ],
   "source": [
    "from spark_intro_solns import transformation_pipeline\n",
    "print(transformation_pipeline(airline_small_rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Now run this pipeline on the full dataset, relax while the processing is done, and enjoy. You rock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataq1, dataq2, dataq3, dataq4 = transformation_pipeline(airline_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 departing airports that have the lowest average delay:[('10930', 60.73560517038778), ('13388', 60.03344481605351), ('13964', 52.38934426229508), ('13424', 49.313011828935394), ('10157', 45.66734211415063), ('14487', 39.08197989172467), ('11002', 34.00552995391705), ('13541', 33.845454545454544), ('10170', 33.21785714285714), ('10165', 31.931818181818183)]\n",
      "Top 10 departing airports that have the highest average delay:[('13127', -2.950089126559715), ('14113', -2.813027744270205), ('11336', -1.7261904761904763), ('10739', -1.7254901960784315), ('10466', -1.625), ('14633', -0.6433649289099526), ('15389', -0.569078947368421), ('11648', -0.48034934497816595), ('11898', -0.32980132450331123), ('12402', -0.30011664722546244)]\n",
      "Top 10 arriving airports that have the lowest average delay:[('14955', 13.0), ('14794', 3.7736625514403292), ('12177', 3.576540755467197), ('14802', 2.8), ('10918', 2.4816176470588234), ('14254', 1.7567185289957568), ('15295', 1.6835443037974684), ('12402', 1.4020996500583236), ('10551', 0.8428745432399513), ('15356', 0.8)]\n",
      "Top 10 arriving airports that have the highest average delay:[('11415', -12.653968253968253), ('12389', -10.4), ('11537', -9.228426395939087), ('13541', -8.772727272727273), ('10581', -8.133451957295375), ('11252', -8.116187989556137), ('12888', -8.018518518518519), ('12007', -7.959183673469388), ('12094', -7.332170880557977), ('10466', -7.1875)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 departing airports that have the lowest average delay:{}\".format(dataq1))\n",
    "\n",
    "print(\"Top 10 departing airports that have the highest average delay:{}\".format(dataq2))\n",
    "\n",
    "print(\"Top 10 arriving airports that have the lowest average delay:{}\".format(dataq3))\n",
    "\n",
    "print(\"Top 10 arriving airports that have the highest average delay:{}\".format(dataq4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
