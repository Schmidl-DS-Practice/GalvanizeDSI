{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null Hypothesis Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem Based Hypothesis Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the most common hypothesis tests are based on the central limit theorem.  Our jumping off point are the simple binomial tests we studied earlier in the week.  By applying the central limit theorem, and occasionally a little ingenuity, we can use the same ideas in more general situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The One Sample Approximate Test of Population Proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in the week we had an example where Matt was testing how frequently he can land kickflips.  \n",
    "\n",
    "Suppose Matt wants to test his hypothesis more rigorously.  Instead of skating for a day, he spends an entire month collecting data.\n",
    "\n",
    "**Let's say** he attempt 100 kickflips a day, for a total of 3100 kickflips, and he land 2531 of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\text{# Kickflips Landed} \\approx Binomial(3100, 0.8) $$\n",
    "\n",
    "In this case, our $N$ is quite large, so it's possible that we have a computer that cannot handle the exact calculations for the binomial distribution (we don't).\n",
    "\n",
    "Luckily the central limit theorem tells us that a binomial with large $N$ is well approximated by a Normal distribution with the appropriate mean and varaince.\n",
    "\n",
    "$$ Binomial(3100, 0.8) \\approx N(3100 \\times 0.8, \\sqrt{3100 \\times 0.8 \\times 0.2}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How did I compute the mean and variance of this normal distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial = stats.binom(n=3100, p=0.80)\n",
    "binomial_mean = 0.8 * 3100\n",
    "binomial_var = 3100 * 0.8 * 0.2\n",
    "normal_approx = stats.norm(binomial_mean, np.sqrt(binomial_var))\n",
    "x = np.linspace(0, 3100, num=3000)\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(16, 6))\n",
    "bar_sizes = [binomial.pmf(i) for i in range(3101)]\n",
    "bars = axs[0].bar(range(3101), bar_sizes, color=\"black\", align=\"center\")\n",
    "axs[0].plot(x, normal_approx.pdf(x), linewidth=3)\n",
    "axs[0].set_xlim(0, 2600)\n",
    "\n",
    "bars = axs[1].bar(range(3101), bar_sizes, color=\"grey\", align=\"center\")\n",
    "axs[1].plot(x, normal_approx.pdf(x), linewidth=3)\n",
    "axs[1].set_xlim(2400, 2600)\n",
    "\n",
    "axs[0].set_title(\"# of Kickflips Landed Under The Null Hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation here is very good, so we can replace the exact Binomail distribution with the approximate Normal distribution.\n",
    "\n",
    "```python\n",
    "binomial_mean = 0.8 * 3100\n",
    "binomial_var = 3100 * 0.8 * 0.2\n",
    "normal_approx = stats.Normal(binomial_mean, np.sqrt(binomial_var))\n",
    "```\n",
    "\n",
    "The p-value for this one month experiment is\n",
    "\n",
    "$$ P(\\geq \\text{ 2531 Kickflips Landed} \\mid \\text{Null Hypothesis} ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = 1 - normal_approx.cdf(2530.5)\n",
    "print(\"p-value for one month kickflip experiment: {:2.2f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "ax.plot(x, normal_approx.pdf(x), linewidth=3)\n",
    "ax.set_xlim(2400, 2600)\n",
    "ax.fill_between(x, normal_approx.pdf(x), \n",
    "                where=(x >= 2530), color=\"red\", alpha=0.5)\n",
    "ax.set_title(\"p-value Reigon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The month of practice has made Matt much more confident in his skill, he should definately move on to another trick.\n",
    "\n",
    "The approximate test for a population proportion (the \"approximate\" because the Binomial distribution is approximated with a Normal) is often called the **z test for a population proportion** because the tables of tail probabilities of normal distributions that poeople would use to look up tail probablities from the normal distribution back in the day were called \"z-tables\".\n",
    "\n",
    "It's an amusing fact that crappy undergrad statistics textbooks still print the mandataory z-tables on thier back covers.  In 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Matt has worked at galvanize 312 days, and rides the bus two ways each day.  He is thinking of buying a car, as observed that a bus is late often, 340 times to be exact.  He would like to purchase the car if the bus is *truly* late more than half the time.  Set up a z-test to decide if Matt should purchase the car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Sample Approximate Test of Population Proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The math in this example is a little more involved, and the Normal approximation here is much more essential than in the previous example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "Now that Matt is confident in his kickflips, he wants to prove that he is better at them than his friend Nick, at least in terms of consistency.\n",
    "\n",
    "On a day of skating together, Matt attempts 80 kickflips, landing 58, before hurting his ankle.  Nick finishes the full 100, and lands 65."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a Skeptical Stance, and Clearly State This Hypothesis.\n",
    "\n",
    "The skptical stance would be that Matt is *not* better than Nick at kickflips, he is at best, equally good.  More precisely, our null hypothesis is that the frequency Matt lands kickflips is *at most* the frequency that Nick lands kickflips.\n",
    "\n",
    "$$ p_M = P(\\text{Matt lands a kickflip}) $$\n",
    "$$ p_N = P(\\text{Nick lands a kickflip}) $$\n",
    "\n",
    "$$ H_0: \\ p_M \\leq p_N $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Probablistic Model of the Stiuation Assuming the Null Hypothesis is True\n",
    "\n",
    "Assuming that each kcikflip Matt tries is independent of all the rest (with the same for Nick), the number of kickflips Matt and Nick land in a fixed number of attempts are Binomial distributed\n",
    "\n",
    "$$ \\text{# of kickflips Matt lands} \\sim Binomial(80, p_M) $$\n",
    "$$ \\text{# of kickflips Nick lands} \\sim Binomial(100, p_N) $$\n",
    "\n",
    "The numbers here as smaller than before, but a Normal approximation is still appropriate for each of these Binomail distributions\n",
    "\n",
    "$$ \\text{# of kickflips Matt lands} \\approx Normal(80p_M, \\sqrt{80p_M(1-p_M)}) $$\n",
    "$$ \\text{# of kickflips Nick lands} \\approx Normal(100p_N, \\sqrt{100p_N(1-p_N)}) $$\n",
    "\n",
    "Now let's do something clever, **we want to compare the rates that Matt and Nick land kickflips**, so let's instead consider the **frequency** that Matt an Nick land kickflips, instead of the actual number they land.  This puts Matt and Nick on equal footing, neccessary since the attempted different numbers of kickflips\n",
    "\n",
    "$$ \\text{Frequency Matt lands kickflips in a sample} \\approx Normal \\left( p_M, \\sqrt{\\frac{p_M(1-p_M)}{80}} \\right) $$\n",
    "$$ \\text{Frequency Nick lands kickflips in a sample} \\approx Normal \\left( p_N, \\sqrt{\\frac{p_N(1-p_N)}{100}} \\right) $$\n",
    "\n",
    "Finally, assuming that Matt landing a kickflip is independent of Nick landing a kickflip, we get a probablistic model for the **difference in sample frequecies**\n",
    "\n",
    "$$ \\text{Difference in sample frequencies between Matt and Nick} \\approx Normal \\left( p_M - p_N, \\sqrt{\\frac{p_N(1-p_N)}{100} + \\frac{p_M(1-p_M)}{80}} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How did I calculate the mean and varaince of this final Normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing In the Null Hypothesis\n",
    "\n",
    "Our null hypothesis is\n",
    "\n",
    "$$ H_0: \\ p_M \\leq p_N $$\n",
    "    \n",
    "In the same way as before, I want to make this hypothesis maximally difficult to reject, but also have specific probabilities to work with (rather than a range).  The most conservative hypothesis is\n",
    "\n",
    "$$ H_0: \\ p_M = p_N \\equiv p$$\n",
    "\n",
    "Which makes our probabalistic model under the null reduce to\n",
    "\n",
    "$$ \\text{Difference in sample frequencies between Matt and Nick} \\approx Normal \\left( 0, \\sqrt{\\frac{p(1-p)}{100} + \\frac{p(1-p)}{80}} \\right) = Normal \\left( 0, \\sqrt{\\frac{180 p(1 - p)}{8000}} \\right) $$\n",
    "\n",
    "The last problem is that we **don't know the true frequencies $p_M$ and $p_N$, nor the assumend shared fequency p**, (if we did, we wouldnt need the statistical test).  Our only resort is to substitute an estimate of the shared value from the sample\n",
    "\n",
    "$$ \\text{Difference in sample frequencies between Matt and Nick} \\approx Normal \\left( 0, \\sqrt{\\frac{180 \\hat p(1- \\hat p)}{8000}} \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_sample_freq = (58.0 + 65.0) / 180\n",
    "shared_sample_variance = 180 * (shared_sample_freq * (1 - shared_sample_freq)) / 8000\n",
    "\n",
    "difference_in_proportions = stats.norm(0, np.sqrt(shared_sample_variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "x = np.linspace(-1, 1, num=250)\n",
    "ax.plot(x, difference_in_proportions.pdf(x), linewidth=3)\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_title(\"Distribution of Difference in Sample Frequencies Assuming $H_0$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decide how Suprised You Need to Be to Reject Your Skeptical Assumption\n",
    "\n",
    "Tricky in this situation, depends on what Matt wants to do with the information.  \n",
    "\n",
    "Matt wants to be a jerk, and rub it in his friends face, but to do so, he better be really sure.  So let's set the rejection threashold pretty stringently: $\\alpha = 0.02$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Two People Kickflips](images/two-kickflips.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Probability of Finding a Result Equally or More Extreme than Actually Observed Assuming the Probabilistic Model You Created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in proportions in the observed sample is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matt_sample_freq = 58.0 / 80\n",
    "nick_sample_freq = 65.0 / 100\n",
    "difference_in_sample_proportions = matt_sample_freq - nick_sample_freq\n",
    "print(\"Difference in sample proportions: {:2.2f}\".format(difference_in_sample_proportions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the p-value for our experiment is\n",
    "\n",
    "$$ P(\\text{Difference in proportions equal to or more extreme than observed} \\mid \\text{Null hypothesis}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = 1 - difference_in_proportions.cdf(difference_in_sample_proportions)\n",
    "print(\"p-value for kickfip frequency comparison: {:2.2f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "x = np.linspace(-1, 1, num=250)\n",
    "ax.plot(x, difference_in_proportions.pdf(x), linewidth=3)\n",
    "ax.fill_between(x, difference_in_proportions.pdf(x), where=(x >= difference_in_sample_proportions),\n",
    "                color=\"red\", alpha=0.5)\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_title(\"p-value Reigon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the p-value to Your Stated Rejection Threshold.\n",
    "\n",
    "We're no where near confident enough in Matt's superior ability to brag, so he should back off that for the moment.\n",
    "\n",
    "The two sample approximate test for population proportions is sometimes called the **two sample z-test**, for similar reasons as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Matt and his friend Mike like to do integral competitions, they get out their calculus textbook, pick a random integral, then race to compute it.  Of course, the win only counts if they get the answer *correct*, which they can check with an online solver.  After 100 integrals, Matt was correct 70% of the time, and Mike was correct 77% of the time.  Is is safe to say that Mike is better at intergration than Matt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Sample Approximate Test of Population Means (Welsh's t-test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Matt and Nick want to see who can kickflip **higher**.  They set up a measuring tape, and each carefully measure the heights of 25 of thier kickflips (in feet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matt_heights = [1.5, 0.9, 0.9, 1.3, 2.1, 1.9, 1.6, 1.4, 2.3, 1.0, 1.5, 0.9, 2.1,\n",
    "                1.3, 1.4, 2.1, 1.2, 1.7, 1.7, 2.0, 1.6, 1.2, 1.6, 1.7, 1.5]\n",
    "nick_heights = [1.9, 1.8, 1.3, 2.3, 1.7, 1.9, 2.7, 2.1, 2.2, 1.5, 1.8, 2.0, 1.1,\n",
    "                1.7, 2.1, 1.5, 2.3, 1.5, 2.0, 2.4, 1.5, 1.4, 1.6, 1.8, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "ax.scatter(matt_heights, np.repeat(0, len(matt_heights)) + np.random.normal(0, 0.1, len(matt_heights)), s=45)\n",
    "ax.scatter(nick_heights, np.repeat(1, len(nick_heights)) + np.random.normal(0, 0.1, len(matt_heights)), s=45)\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_yticklabels([\"Matt\", \"Nick\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like Nick can consistently kickflip higher than Matt, but let's construct a formal test of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a Skeptical Stance, and Clearly State This Hypothesis.\n",
    "\n",
    "Skepticism here is a bit hard to interpret.  A good baseline assumption seems to be \n",
    "\n",
    "> there is no difference in how high Nick and Matt can kickflip.\n",
    "\n",
    "Now there is clearly random variation in each **individual**  kickflip, so a more precise baseline assumption would be\n",
    "\n",
    "> there is no difference in the average heights of kickflips performed by Nick and Matt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Probablistic Model of the Stiuation Assuming the Null Hypothesis is True\n",
    "\n",
    "Our question concerns population averages (is Matt's population average different than Nick's).  Our measurements are **sample averages**, which, from the central limit theorem, we know are approximately normally distributed given the population average\n",
    "\n",
    "$$ \\text{Sample average of Matt's kickflips} \\sim Normal \\left( \\mu_M, \\sqrt{\\frac{\\sigma^2_M}{25}} \\right) $$\n",
    "$$ \\text{Sample average of Nick's kickflips} \\sim Normal \\left( \\mu_N, \\sqrt{\\frac{\\sigma^2_N}{25}} \\right) $$\n",
    "\n",
    "Again, if we are willing to assume that the hight of Matt's individual kickflips are independent from Nick's, then we can compress the important information into one normal distribution\n",
    "\n",
    "$$ \\text{Difference in sample averages} \\sim Normal \\left( \\mu_M - \\mu_N, \\sqrt{\\frac{\\sigma^2_M}{25} + \\frac{\\sigma^2_N}{25}} \\right) $$\n",
    "\n",
    "Under the assumption of the null hypothesis\n",
    "\n",
    "$$ \\text{Difference in sample averages} \\sim Normal \\left( 0, \\sqrt{\\frac{\\sigma^2_M}{25} + \\frac{\\sigma^2_N}{25}} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're in a similar situation to last time, we do not know the population variances.\n",
    "\n",
    "In the prior problem, our assumptions allowed us to compute the relevent varaince from the shared population frequency (which we estimated with the sample frequency).  There are two differences here\n",
    "\n",
    "  - There are two independent varainces, which we have no reason to believe are the same.\n",
    "  - The variances are independent parameters, unrelated to the mean.\n",
    "  \n",
    "In cases where we have to independently estiamte the variance of a normal distribution from the same samples we are testing, this estimation of the variance contributes to uncertenty in our test.  This means that the Normal distribution is then **too precise** to use as a conservative estimate of the p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welch's t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recify the problem, we first convert to a sample statistic whose variance is expected to be $1$\n",
    "\n",
    "$$ \\frac{\\text{Difference in sample averages}}{\\sqrt{\\frac{\\sigma^2_M}{25} + \\frac{\\sigma^2_N}{25}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we still have a similar issue to the two sample test of population proportions, we do not know the population varainces in the denominator of the formula, so our only recourse is to substitute in the sample variances\n",
    "\n",
    "$$ T = \\frac{\\text{Difference in sample averages}}{\\sqrt{\\frac{\\hat \\sigma^2_M}{25} + \\frac{\\hat \\sigma^2_N}{25}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welch_test_statistic(sample_1, sample_2):\n",
    "    numerator = np.mean(sample_1) - np.mean(sample_2)\n",
    "    denominator_sq = (np.var(sample_1) / len(sample_1)) + (np.var(sample_2) / len(sample_2))\n",
    "    return numerator / np.sqrt(denominator_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_statistic = welch_test_statistic(matt_heights, nick_heights)\n",
    "print(\"Welch Test Statistic: {:2.2f}\".format(test_statistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortuantely, this changes the distribution of the test statistic.  Instead of using a normal distribution, we must not use a **Student's t-distribution**, which accounts for the extra uncertenty in estimating the two new parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-distribution always has mean $0$ and varaince $1$, and has one parameter, the **degrees of freedom**.  Smaller degrees of freedom have heavyer tails, with the distribution becoming more normal as the degrees of freedom gets larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [1, 2, 5, 10, 25]\n",
    "x = np.linspace(-3, 3, num=250)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "normal = stats.norm(0, 1)\n",
    "ax.fill_between(x, normal.pdf(x), color=\"grey\", alpha=0.2)\n",
    "for df in dfs:\n",
    "    students = stats.t(df)\n",
    "    ax.plot(x, students.pdf(x), linewidth=2,\n",
    "            label=\"Degree of Freedom: {}\".format(df))\n",
    "ax.legend()\n",
    "ax.set_title(\"Student's t-distributions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Welch's t-test** says that in this situation, the distribution of T is well approximated with a Student's distribution with an appropriate degree of freedom.  The degrees of freedom calculation is complex, enough that it has a name, the [Welch–Satterthwaite equation](https://en.wikipedia.org/wiki/Muriel_Bristol).  The resulting application to our situation results in [Welch's t-test](https://en.wikipedia.org/wiki/Welch's_t-test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def welch_satterhwaithe_df(sample_1, sample_2):\n",
    "    ss1 = len(sample_1)\n",
    "    ss2 = len(sample_2)\n",
    "    df = (\n",
    "        ((np.var(sample_1)/ss1 + np.var(sample_2)/ss2)**(2.0)) / \n",
    "        ((np.var(sample_1)/ss1)**(2.0)/(ss1 - 1) + (np.var(sample_2)/ss2)**(2.0)/(ss2 - 1))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = welch_satterhwaithe_df(nick_heights, matt_heights)\n",
    "print(\"Degrees of Freedom for Welch's Test: {:2.2f}\".format(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, num=250)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "students = stats.t(df)\n",
    "ax.plot(x, students.pdf(x), linewidth=2, label=\"Degree of Freedom: {:2.2f}\".format(df))\n",
    "ax.legend()\n",
    "ax.set_title(\"Distribution of Welsh's Test Statistic Under the Null Hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WARNING:\n",
    "\n",
    "**Also, the above is not always true!**  The $T$ statistic only has a t-distribution **under the assumption that the population distributions are Normal**!  We did *not* have to assume this for *any* other test, but when we need to estimate the variance of the population, we need more structure!\n",
    "\n",
    "If the population is very non-normal, the properties of the t-test **will fail**.  You must have some legitimate a-priori reason to believe the populations are approximately normal to use a t-test!\n",
    "\n",
    "For this reason, many statisticians advise **AGAINST** t-tests these days, preferring non-parametric tests like the signed rank test.  **We will discuss the signed rank test this afternoon!**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decide how Suprised You Need to Be to Reject Your Skeptical Assumption\n",
    "\n",
    "Given the bragging rights at stake here, and how much they value to friendship, Matt and Nick decide to be reasonably skeptical, so take $\\alpha = 0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Probability of Finding a Result Equally or More Extreme than Actually Observed Assuming the Probabilistic Model You Created.\n",
    "\n",
    "An interesting wrinkle here.  We have no prior assumption about whether Nick is better than Matt, or Matt is better than Nick, so a difference could go in either direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, num=250)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "students = stats.t(df)\n",
    "ax.plot(x, students.pdf(x), linewidth=2, label=\"Degree of Freedom: {:2.2f}\".format(df))\n",
    "_ = ax.fill_between(x, students.pdf(x), where=(x >= -test_statistic), color=\"red\", alpha=0.25)\n",
    "_ = ax.fill_between(x, students.pdf(x), where=(x <= test_statistic), color=\"red\", alpha=0.25)\n",
    "ax.legend()\n",
    "ax.set_title(\"p-value Reigon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = students.cdf(test_statistic) + (1 - students.cdf(-test_statistic))\n",
    "print(\"p-value for different average kickflip height: {:2.2f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have enough evidence to conclude that **one of Nick or Matt is better**.\n",
    "\n",
    "Look's like in this case we should be able to conclude that Nick is better, let's test that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_statistic = welch_test_statistic(nick_heights, matt_heights)\n",
    "\n",
    "p_value = 1 - students.cdf(test_statistic)\n",
    "print(\"p-value for Nick average kickflip height greater than Matt: {:2.3f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Nick is clearly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Parametrics: Mann-Whitney Signed Rank Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Mann-Whitney U-test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) is a modern alternative to the classical Student's and Welch's t-test that makes good use of modern computing power.  It makes **no** distributional assumptions (unlike the t-test, which assumes the populations are normal), and can always be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Sums and the Test Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our example of Matt and Nick competing to kickflip higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "ax.scatter(matt_heights, np.repeat(0, len(matt_heights)) + np.random.normal(0, 0.1, len(matt_heights)), s=45)\n",
    "ax.scatter(nick_heights, np.repeat(1, len(nick_heights)) + np.random.normal(0, 0.1, len(matt_heights)), s=45)\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_yticklabels([\"Matt\", \"Nick\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the Mann-Whitney test is to view this as a competition.  We let each of Nick's kicfips compete against all of Matt's kickflips, and see how many times it wins (i.e. how many of Matt's kickflips it beats).  We then add these number of wins up over all of Nick's kickflips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_winning_pairs(sample_1, sample_2):\n",
    "    sample_1, sample_2 = np.array(sample_1), np.array(sample_2)\n",
    "    n_total_wins = 0\n",
    "    for x in sample_1:\n",
    "        n_wins = np.sum(x > sample_2) + 0.5*np.sum(x == sample_2)\n",
    "        n_total_wins += n_wins\n",
    "    return n_total_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nick_wins = count_winning_pairs(nick_heights, matt_heights)\n",
    "matt_wins = count_winning_pairs(matt_heights, nick_heights)\n",
    "print(\"Number of Nick Wins: {}\".format(nick_wins))\n",
    "print(\"Number of Matt Wins: {}\".format(matt_wins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the total number of wins is just the total number of comparisons between one of Matt's kickflips and one of Nicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Number of Wins: {}\".format(nick_wins + matt_wins))\n",
    "print(\"Total Number of Comparisons: {}\".format(\n",
    "    len(nick_heights)*len(matt_heights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of wins we calculated above is called the **Mann-Whitney U Statistic**, or the **Rank Sum Statisitic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The U-Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the hypothesis that Nick is better than Matt, we need to adopt a Null hypothesis.  The Null for the Mann-Whitney test is directly related to which competitor is better.\n",
    "\n",
    "> $H_0$: Matt's Kickflips are equaliy likely to be higher than Nicks as the other way around.  I.e. \n",
    "  \n",
    "  $$P(\\text{Height Nick Kickflip} > \\text{Height Matt Kickflip}) = 0.5$$\n",
    "  \n",
    "As is usual, assuming this null hypothesis is true, the rank-sum statistic assums a known (but complicated) distribution.  This time we can't write down the distribution in any explicit way, but python can calculate p-values using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.mannwhitneyu(nick_heights, matt_heights, alternative=\"greater\")\n",
    "print(\"p-value for Nick > Matt: {:2.3f}\".format(res.pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same result as with the t-test: Nick is clearly better than Matt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Properties of Hypothesis Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic of hypothesis testing can seem awkward and silly when first encountered, and the great number of individual hypothesis tests you need to learn to \"speak the language\" can be overwhelming.  Hopefully our examples have made the idea clear and intuitive.\n",
    "\n",
    "Let's now turn to some higher level properties of hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Distribution of p-values Under the Null Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisher gave the first definition of **p-value**, his original intention was only to devise some measure of **strength of evidence** for a scientific hypothesis.\n",
    "\n",
    "The idea of a *rejection threashold* came later, from Neyman–Pearson.  There idea was to **control the rate of false scientific discoveries**.\n",
    "\n",
    "To explain this, we need to study the distribution of p-values under the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are studying a question, and the null hypothesis is **actually true**.  We collect some data, and compute the p-value of this data under the null hypothesis.  The question we want to address is **how do the computed p-values behave probabalistically?**\n",
    "\n",
    "That is, drawing a different sample will result in a different p-value, how are these p-values distributed with respect to different samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Let's brainstorm.  Assuming the Null Hypothesis is **actually true** in some situations, what should be the distribution of p-values we observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to do this is to construct a simulation, let's write a function that runs a (one-tailed) one sample z-test on data sampled from the null hypothesis distribution, and returns the p-value of the sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_one_sample_z_test(n_simulations, sample_size=100, mu=0.0, sigma=1.0):\n",
    "    normal = stats.norm(mu, sigma)\n",
    "    sampling_distribution = stats.norm(mu, sigma / np.sqrt(sample_size))\n",
    "    p_values = []\n",
    "    for _ in range(n_simulations):\n",
    "        sample = normal.rvs(sample_size)\n",
    "        sample_mean = np.mean(sample)\n",
    "        #sample_variance = np.var(sample)\n",
    "        p_value = 1 - sampling_distribution.cdf(sample_mean)\n",
    "        p_values.append(p_value)\n",
    "    return p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run many simulations, and draw a histogram of the p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = simulate_one_sample_z_test(10**5)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "_ = ax.hist(p_values, bins=100, alpha=0.75, density=True, color=\"grey\")\n",
    "ax.set_title(\"Distribution of p-values Under the Null Hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The p-values from a properly done hypothesis test, in the situation that the null hypothesis is true, are uniformly distributed between 0 and 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controling the False Positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact above gives some insight into the idea behind the rejection threshold $\\alpha$.\n",
    "\n",
    "Recall that we set a rejection threshold *before* running the eperiment, and it is related to the weight of evidence we require before rejecting the null hypothesis.  I.e., we reject the null hypothesis when our computed p-value is less than our rejection threshold.\n",
    "\n",
    "In the situation where the **null hypothesis is actually true**, setting a rejection threshold of, say, $0.05$ ensures that we will **only falsely reject the null hypothesis 5% of the time**.  Falsely rejecting the null hypothesis is called a **false positive**, or a **type one error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "_, _, patches = ax.hist(p_values, bins=100, alpha=0.75, density=True, color=\"grey\")\n",
    "for i in range(6):\n",
    "    patches[i].set_color(\"green\")\n",
    "    \n",
    "ax.set_title(\"False Positive p-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green bars are the tests in our simulation that falsely reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Repeated Experiment Philosophy\n",
    "\n",
    "> If we repeatedly and properly do a hypothesis test with rejection threshold $\\alpha$ in a situation where our research hypothesis is **false**, then we will only **flasely conclude that it is true** at a rate of $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the repeated experiment philosophy is often stated as the basis for hypothesis testing, it is actually **not** how hypothesis testing is used.  A more accurate philosophy is the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worst Case Long Term False Positive Rate\n",
    "\n",
    "> In the worst case situation that **all of our scientific hypothesis are false**, scientists using a rejection threshold of $\\alpha$ for thier experiments will have, in the long term, a false positive rate of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Why is the second interpretation much more reasonable than the first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Failing to reject the null hypothesis when the scientific hypothesis is **true** is called a **false negative** or a **type two error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studying the type two error rate takes some new concepts (mainly statistical power), which will be the subject of tomorrow's lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Testing: The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I am considering writing a paper for the **American Journal of Skateboarding Research**.  I have collected data about kickflips in various situations, different terrain, different weather, different shoes, different warmup routines.  I have various questions I would like to ask of this data:\n",
    "\n",
    "  - Does using a crisp, new board increase the height of kickflips?\n",
    "  - Do dry, sunny days increase the height of kickflips?\n",
    "  - Do broken in (as opposed to new) shoes increase the hieght of kickflips?\n",
    "  - Does an aerobic warmup routine increase the height of kickflips?\n",
    "  \n",
    "Suppose that \n",
    "\n",
    "  - All my alternative hypothesies are, in reality, false. \n",
    "  - I plan to test all these hypothesies using my collected data, and write a paper in **AJSR** if I get a positive result for any of them.\n",
    "  \n",
    "**Question:** If I preform each of these four seperate tests at a threshold of $0.05$, what is the true rate at which I reject **at least one** of these hypothesies and publish a false paper in **AJSR**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation here is a standard probabalistic arguemnt.\n",
    "\n",
    "  - The rate I falsely reject a single hypothesis is $\\alpha$.\n",
    "  - The rate I do **not** falsely reject a single hypothesis is $1 - \\alpha$.\n",
    "  - The rate I do **not** falsely reject **each and every one** of the hypothesies is $(1 - \\alpha)^4$.\n",
    "  - The rate I **do** falsely reject **at least one** of the hypothesies is $1 - (1 - \\alpha)^4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_false_positive_rate = 1 - (1 - 0.05)**4\n",
    "\n",
    "print(\"True combined false positive rate: {:2.2f}\".format(combined_false_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this another way.  Suppose that the four p-values we get are $p_1, p_2$, $p_3$, and $p_4$.  Then we falsely reject the combined hypothesis whenever at least one of the p-values is less than $0.05$\n",
    "\n",
    "$$ min(p_1, p_2, p_3, p_4) < 0.05 $$\n",
    "\n",
    "To keep the false positive rate under control, we would need this to happen only $5\\%$ of the time.\n",
    "\n",
    "We can re-do our simulation and see how often a composite of three hypothesies is falsely rejected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = simulate_one_sample_z_test(4*10**5)\n",
    "p_values_combined = np.array([min(w, x, y, z) for w, x, y, z in \n",
    "                              zip(p_values[::4], p_values[1::4], p_values[2::4], p_values[3::4])])\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "_, _, patches = ax.hist(p_values_combined, bins=100, alpha=0.75, density=True, color=\"grey\")\n",
    "for i in range(6):\n",
    "    patches[i].set_color(\"green\")\n",
    "    \n",
    "ax.set_title(\"False Positive p-values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_false_positive_rate = np.sum(p_values_combined <= 0.05) / float(len(p_values_combined))\n",
    "print(\"Combined False Positive Rate: {:2.2f}\".format(combined_false_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about 18% of the time, we will end up publishing a false paper.  This is **BAD**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferroni Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Bonferroni Correction** is a popular way to rectify the over testing issue.\n",
    "\n",
    "Suppose we want to test a combined hypothesis as a threshold of $\\alpha$.  The bonferroni correction procedure then tests each of the individual hypothesies at a threshold of\n",
    "\n",
    "$$ \\alpha_\\text{Bonferroni} = \\frac{\\alpha}{\\text{# of Hypothesies in Combined Hypothesis}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this fixes the issue in our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_false_positive_rate_corrected = np.sum(\n",
    "    p_values_combined <= 0.05 / 4) / float(len(p_values_combined))\n",
    "print(\"Combined False Positive Rate: {:2.2f}\".format(combined_false_positive_rate_corrected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutiple Testing Over Time: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're studying the impact of changing the layout of your skateboarding website.  The website is intended to map out the skating spots in cities around the country, user's can add spots, photos, and comments.\n",
    "\n",
    "You've made some changes to the comment system, and are hoping that it will draw more forum comments from users.\n",
    "\n",
    "Your plan to test this is to split users of your site into two groups, one group will always see the new layout, and one will always see the old.  You plan to run the site this way for two months, and in the end test whether the users with the new layouts generated more forum comments than those under the old layouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate some data under the **truth** that the new website is **slightly worse**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(132)\n",
    "\n",
    "old_forum_comment_rate = 0.2   # Old Website: Better.\n",
    "new_forum_comment_rate = 0.18  # New Website: Slightly worse.\n",
    "commenters_per_day = stats.poisson(2)\n",
    "\n",
    "# Same Number of Commenters for Each\n",
    "commenters_per_day_old = commenters_per_day.rvs(2*31)\n",
    "commenters_per_day_new = commenters_per_day.rvs(2*31)\n",
    "\n",
    "# \n",
    "comments_per_day_old = [\n",
    "    stats.binom(commenters, old_forum_comment_rate).rvs(1)\n",
    "    for commenters in commenters_per_day_old]\n",
    "comments_per_day_new = [\n",
    "    stats.binom(commenters, new_forum_comment_rate).rvs(1)\n",
    "    for commenters in commenters_per_day_new]\n",
    "\n",
    "cumlative_comments_per_day_old = np.cumsum(comments_per_day_old)\n",
    "cumlative_comments_per_day_new = np.cumsum(comments_per_day_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "\n",
    "x = np.arange(1, 2*31 + 1)\n",
    "ax.plot(x, cumlative_comments_per_day_old, label=\"Old Layout\")\n",
    "ax.plot(x, cumlative_comments_per_day_new, label=\"New Layout\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"Cumulative Number of Comments\")\n",
    "\n",
    "ax.set_title(\"Cumulative Number of Comments over Two Months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we collected makes it look like the new website is **better**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_of_comments_new_layout = np.sum(comments_per_day_new) / float(np.sum(commenters_per_day_new))\n",
    "rate_of_comments_old_layout = np.sum(comments_per_day_old) / float(np.sum(commenters_per_day_old))\n",
    "\n",
    "print(\"Rate of comments, new layout: {:2.2f}\".format(rate_of_comments_new_layout))\n",
    "print(\"Rate of comments, old layout: {:2.2f}\".format(rate_of_comments_old_layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our testing framework tells us the truth, we do not reject the null that the new website is not as good as the old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sample_test_of_population_proportions(commentators_new, comments_new, commentators_old, comments_old):\n",
    "    difference_in_sample_proportions = (comments_new / float(commentators_new)) - (comments_old / float(commentators_old))\n",
    "    overall_proportion = (comments_new + comments_old) / float(commentators_new + commentators_old)\n",
    "    test_varaince = ((comments_new + comments_old) * overall_proportion * (1 - overall_proportion)) / (commentators_new + commentators_old)\n",
    "    test_distribution = stats.norm(0, np.sqrt(test_varaince))\n",
    "    p_value = 1 - test_distribution.cdf(difference_in_sample_proportions)\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = two_sample_test_of_population_proportions(\n",
    "    np.sum(commenters_per_day_new), np.sum(comments_per_day_new), \n",
    "    np.sum(commenters_per_day_old), np.sum(comments_per_day_old)\n",
    ")\n",
    "\n",
    "print(\"p-value for full experiment: {:2.2f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we cheated.  We looked at the data every day, and ran a hypothesis test on that days data, stopping if we ever got a significant result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the sequence of p-values we would get if we run the test every day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumlative_comments_per_day_old = np.cumsum(comments_per_day_old)\n",
    "cumlative_comments_per_day_new = np.cumsum(comments_per_day_new)\n",
    "cumlative_commenters_per_day_old = np.cumsum(commenters_per_day_old)\n",
    "cumlative_commenters_per_day_new = np.cumsum(commenters_per_day_new)\n",
    "\n",
    "p_values = [\n",
    "    two_sample_test_of_population_proportions(\n",
    "        cumlative_commenters_per_day_new[i], \n",
    "        cumlative_comments_per_day_new[i],\n",
    "        cumlative_commenters_per_day_old[i], \n",
    "        cumlative_comments_per_day_old[i]\n",
    "    )\n",
    "    for i in range(1, 2*31)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "ax.plot(range(1, 2*31), p_values)\n",
    "ax.set_title(\"P-values Over Time\")\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"p-value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we peeked at our data any time between the 10'th and 16'th day, we would have made the wrong conclusion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "ax.plot(range(1, 2*31), p_values)\n",
    "ax.axvline(10, linestyle=\"--\")\n",
    "ax.axvline(16, linestyle=\"--\")\n",
    "ax.axvspan(10, 16, alpha=0.25, color='red')\n",
    "ax.set_title(\"P-values Over Time\")\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"p-value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What is the danger in testing every day, and stopping the experiment if a signifigant result is found?  Everything seemed to go ok here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Chi Squared Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear Congruential Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've been contracted to create a small game that will run on an [embedded system](https://en.wikipedia.org/wiki/Embedded_system) (think, for example, a [Tamagotchi](https://en.wikipedia.org/wiki/Tamagotchi) ).  The system does not have many resources, so you have to create your own random number generator.\n",
    "\n",
    "After some research, you hit on a lightweight solution, a [linear congruential generator](https://en.wikipedia.org/wiki/Linear_congruential_generator).  To scope things out, you code a simple generator in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCongruentialGenerator:\n",
    "    \n",
    "    def __init__(self, a, c, modulus, seed):\n",
    "        self._a = a\n",
    "        self._c = c\n",
    "        self._modulus = modulus\n",
    "        self._seed = seed\n",
    "        \n",
    "    def next(self):\n",
    "        next_sample = (self._a * self._seed + self._c) % self._modulus\n",
    "        self._seed = next_sample\n",
    "        return next_sample\n",
    "    \n",
    "    def sample(self, n):\n",
    "        L = []\n",
    "        for _ in range(n):\n",
    "            L.append(lcm.next())\n",
    "        return np.array(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lookup some parameters to use online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcm = LinearCongruentialGenerator(48271, 0, 2**31 - 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to test if you're code really works, which involves assessing the \"randomness\" of your generator.  \n",
    "\n",
    "**Discussion:** How would you assess the quality of your random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your intention in the app is to use the generator to create random six sided dice rolls, so you plan to test whether the generator is appropriate for that application.\n",
    "\n",
    "You have a strong belief that the die is fair, as this code is based on an industry standard method, so your goal here is to detect whether the random number generator is *unfair.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app will only be used in small bursts of activity, so you decide to run these tests on a smallish sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_rolls = lcm.sample(50) % 6\n",
    "dice_freqs = np.bincount(dice_rolls)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "ax.bar(range(1, 7), dice_freqs, align='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chi-Squared Test for Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to arrange our results into a **contingency table**, which compares the expected frequency to the observed frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dice_rolls_contingency_table = pd.DataFrame(\n",
    "    {'expected': np.repeat(8.3, 6) , 'actual': dice_freqs}\n",
    ")\n",
    "print(dice_rolls_contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Chi Squared Test for Distribution**, (also known as the **Chi Squared Test for Goodness of Fit** tests the hypothesis that the **actual** data was not generated from a discrete distributition which has expected frequencies as calcualted.\n",
    "\n",
    ">$H_0$: The data was generated from a dicrete distribution with the given expected frequencies.\n",
    "\n",
    "> $H_{a}$: The data was *not* generated from a discrete distribution with the given expected frequencies.\n",
    "\n",
    "In out situation, a shorter way to express this is\n",
    "\n",
    ">$H_0$: The data generated from the random number generator is consistent with a fair die.\n",
    "\n",
    "> $H_{a}$: The data generated from the random number generator is not consistent with a fair die."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** a point of awkwardness here.  We would like to conclude the die is fair, so really our antagonistic hypothesis should be that the die is unfair.  But assuming the die is unfair does not let us create a probabilistic model for the situation under the null.\n",
    "\n",
    "Unforunately, this is the common logic of Chi squared tests for distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the Contingency Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already done this step.  Good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Test Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the Chi Squared test is to compute the following test statistic:\n",
    "\n",
    "$$ T = \\sum_i \\frac{(O_i - E_i)^2}{E_i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_test_statistic(observeds, expecteds):\n",
    "    numerators = (observeds - expecteds)**2\n",
    "    ratios = numerators / expecteds\n",
    "    return np.sum(ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic of the Chi Squared test is that this quantity follows a certain distribution, the chi squared distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preform the Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chi Squared distribution has one parameter, the degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [1, 2, 3, 4, 5]\n",
    "x = np.linspace(0, 3, num=250)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(16, 3))\n",
    "\n",
    "for df in dfs:\n",
    "    chisq = stats.chi2(df)\n",
    "    ax.plot(x, chisq.pdf(x), linewidth=2,\n",
    "            label=\"Degree of Freedom: {}\".format(df))\n",
    "ax.set_ylim(0, 0.5)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chi squared test provides the degrees of freedom as:\n",
    "    \n",
    "$$ \\text{Number of Cells in the Contingency Table} - \\text{Number of Parameters in Distribution} + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's apply this to our contingency tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dice Rolls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, our degrees of freedom for the Chi Squared distribution is\n",
    "\n",
    "$$ 6 - 1 + 1 = 6 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the test statistic is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = chi_squared_test_statistic(dice_rolls_contingency_table.actual, \n",
    "                               dice_rolls_contingency_table.expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our p-value for the test is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_distribution = stats.chi2(6)\n",
    "p_value = 1 - test_distribution.cdf(T)\n",
    "print(\"p-value for dice rolls: {:2.2f}\".format(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this experiment does *not* give us enough evidence to conclude that our random number generator produces fair rolls.  Looks like we should do more research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Do the other test, of consecutive differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Secret Code to Find Random Seed that Rejects In Forum Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_on_day(seed, burn_in=0):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    old_forum_comment_rate = 0.2   # Old Website: Better.\n",
    "    new_forum_comment_rate = 0.18  # New Website: Slightly worse.\n",
    "    commenters_per_day = stats.poisson(2)\n",
    "\n",
    "    # Same Number of Commenters for Each\n",
    "    commenters_per_day_old = commenters_per_day.rvs(2*31)\n",
    "    commenters_per_day_new = commenters_per_day.rvs(2*31)\n",
    "\n",
    "    # \n",
    "    comments_per_day_old = [\n",
    "        stats.binom(commenters, old_forum_comment_rate).rvs(1)\n",
    "        for commenters in commenters_per_day_old]\n",
    "    comments_per_day_new = [\n",
    "        stats.binom(commenters, new_forum_comment_rate).rvs(1)\n",
    "        for commenters in commenters_per_day_new]\n",
    "\n",
    "    cumlative_comments_per_day_old = np.cumsum(comments_per_day_old)\n",
    "    cumlative_comments_per_day_new = np.cumsum(comments_per_day_new)\n",
    "\n",
    "    p_values = np.array([\n",
    "        two_sample_test_of_population_proportions(\n",
    "            cumlative_commenters_per_day_new[i], cumlative_comments_per_day_new[i],\n",
    "            cumlative_commenters_per_day_old[i], cumlative_comments_per_day_old[i]\n",
    "        )\n",
    "        for i in range(1, 2*31)\n",
    "    ])\n",
    "    \n",
    "    return any(p_values[burn_in:] <= 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(1, 50000):\n",
    "    if reject_on_day(seed, burn_in=15):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(0, 1, size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(222)\n",
    "np.random.uniform(0, 1, size=50)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
