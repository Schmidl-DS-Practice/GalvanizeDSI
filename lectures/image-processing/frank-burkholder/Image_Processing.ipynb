{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Analysis and Featurization\n",
    "\n",
    "Frank Burkholder  \n",
    "Adapted from Austin campus - Joe Gartner and Dan Rupp\n",
    "\n",
    "Objectives:\n",
    "1. Describe how images are represented in computers\n",
    "2. Issues of applying models to images\n",
    "3. Understand basic feature extraction\n",
    "4. Show you a nice example of an image-based capstone, and make the case that figuring out and explaining **why** a machine learning model works can be **more interesting** and **more fun** than just optimizing it to get a good score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are images stored in computers?\n",
    "\n",
    "\n",
    "Images are saved as a matrix of numbers.  Pixels intensities are quantified using various scales.  In each scale the higher the number, the more saturated it is.  \n",
    "* `uint8` is an unsigned, 8 bit integer.  2^8 = 256, so `uint8` values go from 0 - 255.  In a grayscale image, 0 will be black and 255 white.  This is the most common format.\n",
    "* `uint16` also unsigned, 16 bit.  2^16 = 65536.  Twice the memory of a `uint8` value.\n",
    "* `float` a float value (usually 64 bit) between 0-1.  0 black, 1 white.\n",
    "\n",
    "\n",
    "![grayscale](imgs/grey_image.jpg)    \n",
    "\n",
    "    \n",
    "What do you notice about this image?  How is the quality?\n",
    "\n",
    "\n",
    "![deer](imgs/deer_pixeled.jpg)   \n",
    "    \n",
    "What do we change for color images?  \n",
    "\n",
    "Color images are typically 3 equally-sized matrices (rows x columns), where each matrix specifies how much **red**, **blue**, and **green** is present in the image.\n",
    "\n",
    "![RGB](imgs/RGB_channels_separation.png)\n",
    "    \n",
    "<br>\n",
    "\n",
    "Color images are stored in _three dimensional_ matrices. \n",
    "* The first dimension is usually the height dimension starting from the _top_ of the image. \n",
    "* The second dimension is usually the width, starting from the left.\n",
    "* The third dimension is usually the color. This is the \"channel\" dimension.\n",
    "\n",
    "For example, the 0th row (the top), the 0th column (the left), and the 0th color (usually red).   \n",
    "    \n",
    "Images may sometimes have a fourth channel for transparency (\"alpha channel\"), or store the colors in an order other than the standard red-green-blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with images in Python  \n",
    "  \n",
    "There are several tools you can use when working with images in python\n",
    " 1. **Numpy** - this is a standard way to store and work with the image matrix in python\n",
    " 2. **scikit-image** - included with Anaconda it was developed by the SciPy community.  It can be used to do many things we will cover shortly\n",
    " 3. **OpenCV** - there is a Python wrapper of OpenCV which is a C++ library for computer vision.  It is a very powerful tool as it has several pretrained models in it and the structure to allow training of your own classical image models.\n",
    " 4. **PIL** and **Pillow** - Pillow is supposed to be a user friendly version of PIL which is the Python Imaging Library. It adds image processing capabilities to Python.\n",
    "\n",
    "\n",
    "This notebook will use **skimage**.  It has very nice [examples](https://scikit-image.org/docs/dev/auto_examples/) and [documentation](https://scikit-image.org/docs/dev/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline \n",
    "\n",
    "from skimage import io, color, filters\n",
    "from skimage.transform import resize, rotate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin = io.imread('data/coin.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type: {}'.format(type(coin)))\n",
    "print('Shape: {}'.format(coin.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.imshow(coin);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(coin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's zoom in\n",
    "face = coin[75:175,140:240,:]  #it's just an array - you can slice it\n",
    "print(face.shape)\n",
    "io.imshow(face);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunset = io.imread('data/mich_1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Type: {}'.format(type(sunset)))\n",
    "print('Shape: {}'.format(sunset.shape))\n",
    "io.imshow(sunset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the sunset's top left 100 pixel intensities (10 rows x 10 columns) for the first layer (red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunset[:10,:10, 0].reshape(10,10)  # channel 0 is red, 1 is green, 2 is blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the pixel intensities of each of the channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunset_red = sunset[:, :, 0]\n",
    "sunset_green = sunset[:, :, 1]\n",
    "sunset_blue = sunset[:, :, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sunset_red.shape) \n",
    "print(sunset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
    "ax[0].imshow(sunset_red, cmap='gray')\n",
    "ax[0].set_title(\"Red channel saturation\")\n",
    "ax[1].imshow(sunset)\n",
    "ax[1].set_title(\"Original image\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
    "ax[0].imshow(sunset_blue, cmap='gray')\n",
    "ax[0].set_title(\"Blue channel saturation\")\n",
    "ax[1].imshow(sunset)\n",
    "ax[1].set_title(\"Original image\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
    "ax[0].imshow(sunset_green, cmap='gray')\n",
    "ax[0].set_title(\"Green channel saturation\")\n",
    "ax[1].imshow(sunset)\n",
    "ax[1].set_title(\"Original image\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we don't care about color in an image?\n",
    "Make it gray-scale: 1/3rd the size in memory from original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunset_gray = rgb2gray(sunset)\n",
    "print(\"Image shapes:\")\n",
    "print(\"Sunset RGB (3 channel): \", sunset.shape)\n",
    "print(\"Sunset (gray): \", sunset_gray.shape)\n",
    "\n",
    "print(\"\\nMinimum and maximum pixel intensities:\")\n",
    "print(\"Original sunset RGB: \", sunset.min(), \",\", sunset.max())\n",
    "print(\"Sunset gray (grayscale):\", sunset_gray.min(), \",\", sunset_gray.max())\n",
    "io.imshow(sunset_gray);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A common featurization approach for images: flattening/raveling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it into one long row vector\n",
    "sunset_gray_values = np.ravel(sunset_gray)\n",
    "sunset_gray_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(sunset_gray_values, bins=256)\n",
    "ax.set_xlabel('pixel intensities', fontsize=14)\n",
    "ax.set_ylabel('frequency in image', fontsize=14)\n",
    "ax.set_title(\"Sunset image histogram\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the pixel intensities above, can you think of a way to segment the image that would only show the setting sun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_threshold_intensity = 0 # play with this\n",
    "setting_sun = (sunset_gray >= sun_threshold_intensity).astype(int)\n",
    "io.imshow(setting_sun, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization: how big was the setting sun in our image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_sun_pixels = setting_sun.sum()\n",
    "print(f\"The setting sun was represented by {size_sun_pixels} pixels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the coin again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_gray = rgb2gray(coin)\n",
    "coin_gray_values = np.ravel(coin_gray)\n",
    "print(coin_gray_values.shape)\n",
    "io.imshow(coin_gray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(coin_gray_values, bins=256)\n",
    "ax.set_xlabel('pixel intensities', fontsize=14)\n",
    "ax.set_ylabel('frequency in image', fontsize=14)\n",
    "ax.set_title(\"Coin image histogram\", fontsize=16)\n",
    "ax.set_ylim([0, 10000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout on your own\n",
    "Figure out a way to segment the coin in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some things we could learn about the image?\n",
    "\n",
    "If we have a color image how could we find out about aspects of that image?\n",
    " - We can get the mean of each color in the image to get the mood\n",
    " - More complex we can use K-means and look at centroids\n",
    " \n",
    "KMeans clustering is a common way to extract the dominant colors in an image. [The PyImageSearch blog](https://www.pyimagesearch.com/2014/05/26/opencv-python-k-means-color-clustering/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunset.reshape(-1,3).mean(axis=0)  # the average r,g,b value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans  # unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sunset.reshape(-1,3)\n",
    "clf = KMeans(n_clusters=3).fit(X)  # looking for the 3 dominant colors\n",
    "clf.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(clf.labels_)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( clf.labels_.reshape(480, 640) );  # color here means nothing, only labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image transformations\n",
    "\n",
    "Often you need to modify the image before working with it, like **resizing** it or **rotating** it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.imshow(resize(coin, (100,100)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.imshow(rotate(coin, 90));  #skimage to the rescue - try other angles besides 90 degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does ML data look like?\n",
    "\n",
    "What does our data typically look like in data science?\n",
    "\n",
    "| &nbsp;| feature   | feature   | feature   | feature    | \n",
    "| :---- | :---- | :--   | :--   | :--   | \n",
    "| sample 1     |  A    | B | A| B    | \n",
    "| sample 2    |  B   | A     |B     | A| \n",
    "| sample 3    | A| A| A| A     | \n",
    "| sample 4    | B| B    | B| B    |    \n",
    "    \n",
    "\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<details><summary><font size='4'>\n",
    "Q: Why is this an issue with images? (Click)\n",
    "    </font>\n",
    "</summary>\n",
    "    \n",
    "<font size='3'>\n",
    "    <br/>\n",
    " - We have to unravel the image to make it flat losing the relationship of surrounding pixels <br/>\n",
    " - Lighting will affect the pixel values (a car will have a very different set of pixel values if it is cloudy vs sunny)<br/>\n",
    " - Humans are very good at finding shapes and using those to classify a image<br/>\n",
    " - You can think of shapes/edges as the difference in adjoining pixels <br/>\n",
    "    \n",
    "</details>\n",
    "<br/><br/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization - Looking for edges in images\n",
    "\n",
    "It *may* help our ML model to give it edges, instead of pixel intensities, to train and predict on.\n",
    "\n",
    "This is called [edge detection](https://en.wikipedia.org/wiki/Edge_detection) and there are a variety of ways to do it.\n",
    "\n",
    "A straight-forward approach is to use the gradient (rate of change) of the pixel intensities.\n",
    "\n",
    "Looking for the direction of \"color\" change.  (I will be using grayscale from here on out)\n",
    "\n",
    "Looking for the change in gradient for a given pixel to the ones immediately above and below and to the left and right can be described with the following function.  \n",
    "   \n",
    "\n",
    "![gradient](imgs/gradient_fromula.png)\n",
    "\n",
    "![grid](imgs/image_grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the calculations\n",
    " - Magnitude (L2-norm): \n",
    "   * $ g = \\sqrt{g_x^2 + g_y^2}$\n",
    " - Direction is the arctangent of the ration of the partial derivatives:  \n",
    "   * $ \\theta = arctan(g_y/g_x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus giving us \n",
    "\n",
    "![solved](imgs/solved.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is computationally slow it can be approximated by applying what is called a **convolution** or a vector of the needed form.  For the x vector that would be `[-1,0,1]`.  Which given the above example will give us the value of `-50`.  This is sometimes referred to as edge detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobel operator\n",
    "\n",
    "The convolutions are used to gain information about the pixels surrounding the single pixel.  You can slide a convolution over a image and end up with a new image where every pixel now represents numerically the surrounding pixels.   \n",
    "One of the more well known ones is the Sobel.  It has a mathematical make up like below:\n",
    "\n",
    "\n",
    "![sobel](imgs/sobel.png)\n",
    "\n",
    "\n",
    "The magnitude is calculated between these two edge detectors to get the final result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_img = filters.sobel(coin_gray)\n",
    "io.imshow(sobel_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with a image that shows the presence of edges as distinct numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_img[0:10,140:150].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different filters that have varying mathematics behind them however the idea that we are getting data on the difference of one pixel to another is followed.  Getting the difference between the pixels is important as this minimizes the issue of different lighting.  Also normalization of images first is usually recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a blur filter\n",
    "io.imshow(gaussian(coin_gray, sigma=3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "1. Describe how images are represented in computers\n",
    "2. Issues of applying models to images\n",
    "3. Understand basic feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "Let's check out an amazing image-based capstone 1:  [What the fish?!(betta)](https://github.com/joeshull/what_the_fish_betta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
