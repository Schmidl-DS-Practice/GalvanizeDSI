{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis Notebook\n",
    "F. Burkholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ggplot style\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# respect for the audience\n",
    "import matplotlib\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "\n",
    "# example of keyword argument unpacking\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show an example of an orthogonal transformation (a rotation)\n",
    "Conceptually how PCA takes potentially correlated features and tranforms them into uncorrelated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple linear dataset\n",
    "m = 2 # slope\n",
    "b = 0 # intercept\n",
    "x1 = np.random.uniform(-5, 5, size = 20)\n",
    "x2 = m * x1 + b\n",
    "\n",
    "# reshape to column vectors or arbitrary length\n",
    "x1 = x1.reshape((-1,1)) \n",
    "x2 = x2.reshape((-1,1))\n",
    "\n",
    "# make it into a matrix\n",
    "X = np.hstack((x1, x2))\n",
    "\n",
    "print(\"x1\\tx2\")\n",
    "for x1, x2 in X:\n",
    "    print(\"{0:0.1f}\\t{1:0.1f}\".format(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "ax.plot(X[:,0], X[:,1], marker='o', markersize=10, color='k', linewidth=0.5, linestyle='-')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "low = X.min() - 1\n",
    "high = X.max() + 1\n",
    "ax.set_xlim([low, high])\n",
    "ax.set_ylim([low, high])\n",
    "ax.set_aspect('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine its angle from the x-axis\n",
    "theta = np.arctan(m)\n",
    "print(\"The line is {0:0.2f} radians from the x1-axis, or {1:0.1f} degrees.\".format(theta, np.degrees(theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform \"orthogonal transformation\" of the data, namely a [rotation.](https://en.wikipedia.org/wiki/Rotation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the rotation matrix, R\n",
    "# use -theta to rotate it back to the X1 axis\n",
    "R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "              [np.sin(theta), np.cos(theta)]])\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate the data\n",
    "XR = np.dot(X, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot XR and X\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "ax.plot(X[:,0], X[:,1], marker='o', markersize=10, color='k', linewidth=0.5, linestyle='-', label='X')\n",
    "ax.plot(XR[:,0], XR[:,1], marker='o', markersize=10, color='b', linewidth=0.5, linestyle='-', label='XR')\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "low = min(X.min(), XR.min()) - 1\n",
    "high = max(X.max(), XR.max()) + 1\n",
    "ax.set_xlim([low, high])\n",
    "ax.set_ylim([low, high])\n",
    "ax.legend()\n",
    "ax.set_aspect('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with this transformation, data that once required feature x1 and x2 to describe its variance now has all of its variance described by the new x1R axis.  So dimensionality has been reduced, but all the variance in the data has been maintained.  We could build a model using only the one feature and it should perform just as well (and perhaps better), than the two feature model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How-to-PCA\n",
    "1) Standardize columns  \n",
    "2) Create covariance (correlation if standardized) matrix  \n",
    "3) Find the eigenvectors and eigenvalues of the covariance/correlation matrix  \n",
    "4) The eigenvectors are the principal components  \n",
    "\n",
    "Will demonstrate using Numpy and Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column means: \", X.mean(axis=0)) # average of the columns\n",
    "print(\"Column stddevs: \", X.std(axis=0, ddof=1)) # std dev. of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize using numpy\n",
    "X_std = (X - X.mean(axis=0))/X.std(axis=0, ddof=1)\n",
    "print(X_std) #ugh, so many digits\n",
    "print()\n",
    "print(np.around(X_std, 2)) # better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that column means are 0, standard deviation of 1\n",
    "print(X_std.mean(axis=0))\n",
    "print(X_std.std(axis=0, ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could also use sklearn's standard scalar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "scaler.fit(X)\n",
    "print(\"The means of the columns in X are: {0}\".format(scaler.mean_.round(2)))\n",
    "print(\"The standard deviations of the columns in X are: {0}\".format(np.sqrt(scaler.var_).round(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure data is standardized\n",
    "X_std_ss = scaler.transform(X)\n",
    "print(X_std_ss.mean(axis=0))\n",
    "print(X_std_ss.std(axis=0, ddof=1)) #wat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_std_ss.std(axis=0, ddof=0)) #oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently standard scalar calculates variance only for a population, not a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok back from standardizing\n",
    "# plot it\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,6))\n",
    "ax.scatter(X_std[:,0], X_std[:,1], marker='o', color='k')\n",
    "ax.set_xlabel('x1_std')\n",
    "ax.set_ylabel('x2_std')\n",
    "low = X_std.min() - 1\n",
    "high = X_std.max() + 1\n",
    "ax.set_xlim([low, high])\n",
    "ax.set_ylim([low, high])\n",
    "ax.set_aspect('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create covariance/correlation matrix\n",
    "Covariance if just de-meaned, correlation if standardized too (more typical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "A = 1/(N-1)*np.dot(X_std.T, X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the eigenvectors and eigenvalues (with numpy)\n",
    "See [documentation](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.eig.html).  Structure of the returned eigenvectors is not intuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will keep only the first eigenvector, associated with eigenvalue 2\n",
    "pc1 = np.array([[eig_vecs[0][0], eig_vecs[1][0]]]).T\n",
    "print(pc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com) for reconstructing your data matrix from principal components manually (in numpy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn - example on the Iris dataset\n",
    "Iris dataset described [here](https://en.wikipedia.org/wiki/Iris_flower_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from code source: Gaël Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features (there are a total of 4)\n",
    "y = iris.target\n",
    "\n",
    "print(\"X\\t\\ty\")\n",
    "for rowX, rowy in zip(X,y):\n",
    "    print(\"{0}\\t{1}\".format(rowX, rowy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# Plot the points\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,\n",
    "            edgecolor='k')\n",
    "ax.set_xlabel('Sepal length')\n",
    "ax.set_ylabel('Sepal width')\n",
    "ax.set_title('Plotting first two columns of data')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, when we just used the first two columns of data, we didn't see much separation of the classes in our 2d plot.  If we had done PCA first, where we used PCA to reduce it down to two dimensions, maybe we'd see more separation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see a 2d representation of the data using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "\n",
    "print(\"First 3 rows of the iris data:\")\n",
    "print(\"X1\\tX2\\tX3\\tX4\")\n",
    "for i in range(3):\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\".format(iris.data[i,0], iris.data[i,1], iris.data[i,2], iris.data[i,3]))\n",
    "\n",
    "X_scaled = scaler.fit_transform(iris.data) # standardize data\n",
    "\n",
    "pca = PCA(n_components=2) #pca object\n",
    "X_pca = pca.fit_transform(X_scaled) # from 4 features to 2 PCA features\n",
    "\n",
    "print(\"\\nData after PCA into 2 components\")\n",
    "print(\"PC1\\tPC2\")\n",
    "for i in range(3):\n",
    "    print(\"{0:0.1f}\\t{1:0.1f}\".format(X_pca[i,0], X_pca[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How are each of the 2 principal components defined?\n",
    "# Here are the loadings (how the original features load on to the principal components)\n",
    "pca.components_.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1st principal component (PC1) is 0.52 $\\cdot$ X1 + -0.26 $\\cdot$ X2 + 0.58 $\\cdot$ X3 + 0.57 $\\cdot$ X4  \n",
    "The 2nd principal component (PC2) is 0.37 $\\cdot$ X1 + 0.93 $\\cdot$ X2 + 0.02 $\\cdot$ X3 + 0.07 $\\cdot$ X4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are these components orthogonal?  (They should be!)\n",
    "np.dot(pca.components_[0], pca.components_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y,\n",
    "           cmap=plt.cm.Set1, edgecolor='k', s=40)\n",
    "ax.set_title(\"First two PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector (PC1)\")\n",
    "ax.set_ylabel(\"2nd eigenvector (PC2)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using features derived from linear combinations of all the features, this PCA 2d representation does a better job showing the structure in the data (the separations of the classes) than just using any two features.  \n",
    "\n",
    "But did we make a sacrifice by going down to two dimensions (from four) using PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evr = pca.explained_variance_ratio_\n",
    "print(evr)\n",
    "print(\"The 2 principal components explain {0:0.1f}%\"\n",
    "      \" of the variance in the original data.\".format(evr.sum()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ That's really good!  A rule of thumb is to pick your number of components to explain at least 90% of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree plots (how to pick the number of principal components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import decomposition, datasets\n",
    "\n",
    "pca = decomposition.PCA() # not setting number of components, which means \n",
    "                          # we keep them all!\n",
    "\n",
    "digits = datasets.load_digits() # using MNIST\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(\"There are {0} rows of data.\".format(X_digits.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a digit and target\n",
    "img = 11\n",
    "print(X_digits[img])\n",
    "print(\"\\nThe images are {0} in shape.\".format(X_digits[img].shape))\n",
    "print(\"\\nEach value in the image is of type {0}\".format(type(X_digits[img][0])))\n",
    "print(\"(Though they look a lot like 4 bit numbers.)\")\n",
    "plt.imshow(X_digits[img].reshape((8,8)), cmap='gray')\n",
    "print(\"\\nNumber in image: \", y_digits[img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not going to scale the images, because all pixel intensities\n",
    "# are already on the same scale\n",
    "\n",
    "pca.fit(X_digits)\n",
    "\n",
    "# plot explained variance ratio in a scree plot\n",
    "plt.figure(1, figsize=(8, 6))\n",
    "plt.clf()\n",
    "plt.axes([.2, .2, .7, .7])\n",
    "plt.plot(pca.explained_variance_, linewidth=2, color='red')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance_');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variance = np.sum(pca.explained_variance_)\n",
    "cum_variance = np.cumsum(pca.explained_variance_)\n",
    "prop_var_expl = cum_variance/total_variance\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.plot(prop_var_expl, color='red', linewidth=2, label='Explained variance')\n",
    "ax.axhline(0.9, label='90% goal', linestyle='--', color=\"black\", linewidth=1)\n",
    "ax.set_ylabel('cumulative prop. of explained variance')\n",
    "ax.set_xlabel('number of principal components')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Around 20 principal components explains about 90% of the variance in the handwritten digits.\n",
    "So, we could train a model on just those 20 features, instead of the original 64, and do as well or maybe better on unseen data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
