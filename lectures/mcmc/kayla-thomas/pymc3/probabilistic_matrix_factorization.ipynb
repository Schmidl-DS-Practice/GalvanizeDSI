{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Matrix Factorization for Making Personalized Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model discussed in this analysis was developed by Ruslan Salakhutdinov and Andriy Mnih. All of the code and supporting text, when not referenced, is the original work of [Mack Sweeney](https://www.linkedin.com/in/macksweeney).\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Say I download a handbook of a hundred jokes, and I'd like to know very quickly which ones will be my favorite. So maybe I read a few, I laugh, I read a few more, I stop laughing, and I indicate on a scale of -10 to 10 how funny I thought each joke was. Maybe I do this for 5 jokes out of the 100. Now I go to the back of the book, and there's a little program included for calculating my preferences for all the other jokes. I enter in my preference numbers and shazam! The program spits out a list of all 100 jokes, sorted in the order I'll like them. That certainly would be nice. Today we'll write a program that does exactly this.\n",
    "\n",
    "We'll start out by getting some intuition for how our model will work. Then we'll formalize our intuition. Afterwards, we'll examine the dataset we are going to use. Once we have some notion of what our data looks like, we'll define some baseline methods for predicting preferences for jokes. Following that, we'll look at Probabilistic Matrix Factorization (PMF), which is a more sophisticated Bayesian method for predicting preferences. Having detailed the PMF model, we'll use PyMC3 for MAP estimation and MCMC inference. Finally, we'll compare the results obtained with PMF to those obtained from our baseline methods and discuss the outcome.\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Normally if we want recommendations for something, we try to find people who are similar to us and ask their opinions. If Bob, Alice, and Monty are all similar to me, and they all like knock-knock jokes, I'll probably like knock-knock jokes. Now this isn't always true. It depends on what we consider to be \"similar\". In order to get the best bang for our buck, we really want to look for people who have the most similar sense of humor. Humor being a complex beast, we'd probably like to break it down into something more understandable. We might try to characterize each joke in terms of various factors. Perhaps jokes can be dry, sarcastic, crude, sexual, political, etc. Now imagine we go through our handbook of jokes and assign each joke a rating in each of the categories. How dry is it? How sarcastic is it? How much does it use sexual innuendos? Perhaps we use numbers between 0 and 1 for each category. Intuitively, we might call this the joke's humor profile.\n",
    "\n",
    "Now let's suppose we go back to those 5 jokes we rated. At this point, we can get a richer picture of our own preferences by looking at the humor profiles of each of the jokes we liked and didn't like. Perhaps we take the averages across the 5 humor profiles and call this our ideal type of joke. In other words, we have computed some notion of our inherent _preferences_ for various types of jokes. Suppose Bob, Alice, and Monty all do the same. Now we can compare our preferences and determine how similar each of us really are. I might find that Bob is the most similar and the other two are still more similar than other people, but not as much as Bob. So I want recommendations from all three people, but when I make my final decision, I'm going to put more weight on Bob's recommendation than those I get from Alice and Monty.\n",
    "\n",
    "While the above procedure sounds fairly effective as is, it also reveals an unexpected additional source of information. If we rated a particular joke highly, and we know its humor profile, we can compare with the profiles of other jokes. If we find one with very close numbers, it is probable we'll also enjoy this joke. Both this approach and the one above are commonly known as _neighborhood approaches_. Techniques that leverage both of these approaches simultaneously are often called _collaborative filtering_ [[1]](http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf). The first approach we talked about uses user-user similarity, while the second uses item-item similarity. Ideally, we'd like to use both sources of information. The idea is we have a lot of items available to us, and we'd like to work together with others to filter the list of items down to those we'll each like best. My list should have the items I'll like best at the top and those I'll like least at the bottom. Everyone else wants the same. If I get together with a bunch of other people, we all read 5 jokes, and we have some efficient computational process to determine similarity, we can very quickly order the jokes to our liking.\n",
    "\n",
    "## Formalization\n",
    "\n",
    "Let's take some time to make the intuitive notions we've been discussing more concrete. We have a set of $M$ jokes, or _items_ ($M = 100$ in our example above). We also have $N$ people, whom we'll call _users_ of our recommender system. For each item, we'd like to find a $D$ dimensional factor composition (humor profile above) to describe the item. Ideally, we'd like to do this without actually going through and manually labeling all of the jokes. Manual labeling would be both slow and error-prone, as different people will likely label jokes differently. So we model each joke as a $D$ dimensional vector, which is its latent factor composition. Furthermore, we expect each user to have some preferences, but without our manual labeling and averaging procedure, we have to rely on the latent factor compositions to learn $D$ dimensional latent preference vectors for each user. The only thing we get to observe is the $N \\times M$ ratings matrix $R$ provided by the users. Entry $R_{ij}$ is the rating user $i$ gave to item $j$. Many of these entries may be missing, since most users will not have rated all 100 jokes. Our goal is to fill in the missing values with predicted ratings based on the latent variables $U$ and $V$. We denote the predicted ratings by $R_{ij}^*$. We also define an indicator matrix $I$, with entry $I_{ij} = 0$ if $R_{ij}$ is missing and $I_{ij} = 1$ otherwise.\n",
    "\n",
    "So we have an $N \\times D$ matrix of user preferences which we'll call $U$ and an $M \\times D$ factor composition matrix we'll call $V$. We also have a $N \\times M$ rating matrix we'll call $R$. We can think of each row $U_i$ as indications of how much each user prefers each of the $D$ latent factors. Each row $V_j$ can be thought of as how much each item can be described by each of the latent factors. In order to make a recommendation, we need a suitable prediction function which maps a user preference vector $U_i$ and an item latent factor vector $V_j$ to a predicted ranking. The choice of this prediction function is an important modeling decision, and a variety of prediction functions have been used. Perhaps the most common is the dot product of the two vectors, $U_i \\cdot V_j$ [[1]](http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf).\n",
    "\n",
    "To better understand CF techniques, let us explore a particular example. Imagine we are seeking to recommend jokes using a model which infers five latent factors, $V_j$, for $j = 1,2,3,4,5$. In reality, the latent factors are often unexplainable in a straightforward manner, and most models make no attempt to understand what information is being captured by each factor.  However, for the purposes of explanation, let us assume the five latent factors might end up capturing the humor profile we were discussing above. So our five latent factors are: dry, sarcastic, crude, sexual, and political. Then for a particular user $i$, imagine we infer a preference vector $U_i = <0.2, 0.1, 0.3, 0.1, 0.3>$. Also, for a particular item $j$, we infer these values for the latent factors: $V_j = <0.5, 0.5, 0.25, 0.8, 0.9>$. Using the dot product as the prediction function, we would calculate 0.575 as the ranking for that item, which is more or less a neutral preference given our -10 to 10 rating scale.\n",
    "\n",
    "$$ 0.2 \\times 0.5 + 0.1 \\times 0.5 + 0.3 \\times 0.25 + 0.1 \\times 0.8 + 0.3 \\times 0.9 = 0.575 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The [v1 Jester dataset](http://eigentaste.berkeley.edu/dataset/) provides something very much like the handbook of jokes we have been discussing. The original version of this dataset was constructed in conjunction with the development of the [Eigentaste recommender system](http://eigentaste.berkeley.edu/about.html) [[2]](http://goldberg.berkeley.edu/pubs/eigentaste.pdf). At this point in time, v1 contains over 4.1 million continuous ratings in the range [-10, 10] of 100 jokes from 73,421 users. These ratings were collected between Apr. 1999 and May 2003. In order to reduce the training time of the model for illustrative purposes, 1,000 users who have rated all 100 jokes will be selected randomly. We will implement a model that is suitable for collaborative filtering on this data and evaluate it in terms of root mean squared error (RMSE) to validate the results.\n",
    "\n",
    "Let's begin by exploring our data. We want to get a general feel for what it looks like and a sense for what sort of patterns it might contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.08</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>6.36</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-9.66</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-5.34</td>\n",
       "      <td>8.88</td>\n",
       "      <td>9.22</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>-4.95</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>7.86</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.17</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-8.50</td>\n",
       "      <td>-7.09</td>\n",
       "      <td>-4.32</td>\n",
       "      <td>-8.69</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-6.65</td>\n",
       "      <td>-1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>-6.89</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>-2.18</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-9.08</td>\n",
       "      <td>-5.05</td>\n",
       "      <td>-3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.84</td>\n",
       "      <td>3.16</td>\n",
       "      <td>9.17</td>\n",
       "      <td>-6.21</td>\n",
       "      <td>-8.16</td>\n",
       "      <td>-1.70</td>\n",
       "      <td>9.27</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-5.19</td>\n",
       "      <td>-4.42</td>\n",
       "      <td>...</td>\n",
       "      <td>7.23</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-5.68</td>\n",
       "      <td>-3.16</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.79</td>\n",
       "      <td>-3.54</td>\n",
       "      <td>-9.42</td>\n",
       "      <td>-6.89</td>\n",
       "      <td>-8.74</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-5.29</td>\n",
       "      <td>-8.93</td>\n",
       "      <td>-7.86</td>\n",
       "      <td>-1.60</td>\n",
       "      <td>...</td>\n",
       "      <td>4.37</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>4.17</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-3.40</td>\n",
       "      <td>-4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.31</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-7.23</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>...</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-3.30</td>\n",
       "      <td>3.45</td>\n",
       "      <td>5.44</td>\n",
       "      <td>4.08</td>\n",
       "      <td>2.48</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1     2     3     4     5     6     7     8     9    10  ...     91  \\\n",
       "0  4.08 -0.29  6.36  4.37 -2.38 -9.66 -0.73 -5.34  8.88  9.22  ...   2.82   \n",
       "1 -6.17 -3.54  0.44 -8.50 -7.09 -4.32 -8.69 -0.87 -6.65 -1.80  ...  -3.54   \n",
       "2  6.84  3.16  9.17 -6.21 -8.16 -1.70  9.27  1.41 -5.19 -4.42  ...   7.23   \n",
       "3 -3.79 -3.54 -9.42 -6.89 -8.74 -0.29 -5.29 -8.93 -7.86 -1.60  ...   4.37   \n",
       "4  1.31  1.80  2.57 -2.38  0.73  0.73 -0.97  5.00 -7.23 -1.36  ...   1.46   \n",
       "\n",
       "     92    93    94    95    96    97    98    99   100  \n",
       "0 -4.95 -0.29  7.86 -0.19 -2.14  3.06  0.34 -4.32  1.07  \n",
       "1 -6.89 -0.68 -2.96 -2.18 -3.35  0.05 -9.08 -5.05 -3.45  \n",
       "2 -1.12 -0.10 -5.68 -3.16 -3.35  2.14 -0.05  1.31  0.00  \n",
       "3 -0.29  4.17 -0.29 -0.29 -0.29 -0.29 -0.29 -3.40 -4.95  \n",
       "4  1.70  0.29 -3.30  3.45  5.44  4.08  2.48  4.51  4.66  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "%precision 4\n",
    "plt.style.use('bmh')\n",
    "DATA_DIR = './data'\n",
    "\n",
    "data = pd.read_csv(os.path.join(DATA_DIR, 'jester-dataset-v1-dense-first-1000.csv'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7UAAAGcCAYAAAAGZYfiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZKkhmVSK0mSJElqWCa1kiRJkqSG9f8BdAXMd5+WfNkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6da9219160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the ratings from the DataFrame\n",
    "all_ratings = np.ndarray.flatten(data.values)\n",
    "ratings = pd.Series(all_ratings)\n",
    "\n",
    "# Plot histogram and density.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "ratings.plot(kind='density', ax=ax1, grid=False)\n",
    "ax1.set_ylim(0, 0.08)\n",
    "ax1.set_xlim(-11, 11)\n",
    "\n",
    "# Plot histogram\n",
    "ratings.plot(kind='hist', ax=ax2, bins=20, grid=False)\n",
    "ax2.set_xlim(-11, 11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100000.000000\n",
       "mean          0.996219\n",
       "std           5.265215\n",
       "min          -9.950000\n",
       "25%          -2.860000\n",
       "50%           1.650000\n",
       "75%           5.290000\n",
       "max           9.420000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This must be a decent batch of jokes. From our exploration above, we know most ratings are in the range -1 to 10, and positive ratings are more likely than negative ratings. Let's look at the means for each joke to see if we have any particularly good (or bad) humor here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6da91fcb00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA50AAAF7CAYAAABRru/8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUIIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6dc859ccf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joke_means = data.mean(axis=0)\n",
    "joke_means.plot(kind='bar', grid=False, figsize=(16, 6),\n",
    "                title=\"Mean Ratings for All 100 Jokes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the majority of the jokes generally get positive feedback from users, there are definitely a few that stand out as poor humor. Let's take a look at the worst and best joke, just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The worst joke:\n",
      "---------------\n",
      "How many teddybears does it take to change a lightbulb?\n",
      "\n",
      "It takes only one teddybear, but it takes a whole lot of lightbulbs.\n",
      "\n",
      "\n",
      "The best joke:\n",
      "--------------\n",
      "*A radio conversation of a US naval \n",
      "ship with Canadian authorities ... *\n",
      "\n",
      "Americans: Please divert your course 15 degrees to the North to avoid a\n",
      "collision.\n",
      "\n",
      "Canadians: Recommend you divert YOUR course 15 degrees to the South to \n",
      "avoid a collision.\n",
      "\n",
      "Americans: This is the Captain of a US Navy ship.  I say again, divert \n",
      "YOUR course.\n",
      "\n",
      "Canadians: No.  I say again, you divert YOUR course.\n",
      "\n",
      "Americans: This is the aircraft carrier USS LINCOLN, the second largest ship in the United States' Atlantic Fleet. We are accompanied by three destroyers, three cruisers and numerous support vessels. I demand that you change your course 15 degrees north, that's ONE FIVE DEGREES NORTH, or counter-measures will be undertaken to ensure the safety of this ship.\n",
      "\n",
      "Canadians: *This is a lighthouse.  Your call*.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Worst and best joke?\n",
    "worst_joke_id = joke_means.argmin()\n",
    "best_joke_id = joke_means.argmax()\n",
    "\n",
    "# Let's see for ourselves. Load the jokes.\n",
    "with open(os.path.join(DATA_DIR, 'jokes.json')) as buff:\n",
    "    joke_dict = json.load(buff)\n",
    "\n",
    "print('The worst joke:\\n---------------\\n%s\\n' % joke_dict[worst_joke_id])\n",
    "print('The best joke:\\n--------------\\n%s' % joke_dict[best_joke_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sense to me. We now know there are definite popularity differences between the jokes. Some of them are simply funnier than others, and some are downright lousy. Looking at the joke means allowed us to discover these general trends. Perhaps there are similar trends across users. It might be the case that some users are simply more easily humored than others. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA50AAAFlCAYAAABocowXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf9f8ZxKjcxYZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6dc859c2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_means = data.mean(axis=1)\n",
    "_, ax = plt.subplots(figsize=(16, 6))\n",
    "user_means.plot(kind='bar', grid=False, ax=ax,\n",
    "                title=\"Mean Ratings for All 1000 Users\")\n",
    "ax.set_xticklabels('')  # 1000 labels is nonsensical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see even more significant trends here. Some users rate nearly everything highly, and some (though not as many) rate nearly everything negatively. These observations will come in handy when considering models to use for predicting user preferences on unseen jokes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "Having explored the data, we're now ready to dig in and start addressing the problem. We want to predict how much each user is going to like all of the jokes he or she has not yet read.\n",
    "\n",
    "\n",
    "### Baselines\n",
    "\n",
    "Every good analysis needs some kind of baseline methods to compare against. It's difficult to claim we've produced good results if we have no reference point for what defines \"good\". We'll define three very simple baseline methods and find the RMSE using these methods. Our goal will be to obtain lower RMSE scores with whatever model we produce.\n",
    "\n",
    "#### Uniform Random Baseline\n",
    "\n",
    "Our first baseline is about as dead stupid as you can get. Every place we see a missing value in $R$, we'll simply fill it with a number drawn uniformly at random in the range [-10, 10]. We expect this method to do the worst by far.\n",
    "\n",
    "$$R_{ij}^* \\sim Uniform$$\n",
    "\n",
    "#### Global Mean Baseline\n",
    "\n",
    "This method is only slightly better than the last. Wherever we have a missing value, we'll fill it in with the mean of all observed ratings.\n",
    "\n",
    "$$\\text{global_mean} = \\frac{1}{N \\times M} \\sum_{i=1}^N \\sum_{j=1}^M I_{ij}(R_{ij})$$\n",
    "\n",
    "$$R_{ij}^* = \\text{global_mean}$$\n",
    "\n",
    "#### Mean of Means Baseline\n",
    "\n",
    "Now we're going to start getting a bit smarter. We imagine some users might be easily amused, and inclined to rate all jokes more highly. Other users might be the opposite. Additionally, some jokes might simply be more witty than others, so all users might rate some jokes more highly than others in general. We can clearly see this in our graph of the joke means above. We'll attempt to capture these general trends through per-user and per-joke rating means. We'll also incorporate the global mean to smooth things out a bit. So if we see a missing value in cell $R_{ij}$, we'll average the global mean with the mean of $U_i$ and the mean of $V_j$ and use that value to fill it in.\n",
    "\n",
    "$$\\text{user_means} = \\frac{1}{M} \\sum_{j=1}^M I_{ij}(R_{ij})$$\n",
    "\n",
    "$$\\text{joke_means} = \\frac{1}{N} \\sum_{i=1}^N I_{ij}(R_{ij})$$\n",
    "\n",
    "$$R_{ij}^* = \\frac{1}{3} \\left(\\text{user_means}_i + \\text{ joke_means}_j + \\text{ global_mean} \\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Create a base class with scaffolding for our 3 baselines.\n",
    "\n",
    "def split_title(title):\n",
    "    \"\"\"Change \"BaselineMethod\" to \"Baseline Method\".\"\"\"\n",
    "    words = []\n",
    "    tmp = [title[0]]\n",
    "    for c in title[1:]:\n",
    "        if c.isupper():\n",
    "            words.append(''.join(tmp))\n",
    "            tmp = [c]\n",
    "        else:\n",
    "            tmp.append(c)\n",
    "    words.append(''.join(tmp))\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "class Baseline(object):\n",
    "    \"\"\"Calculate baseline predictions.\"\"\"\n",
    "\n",
    "    def __init__(self, train_data):\n",
    "        \"\"\"Simple heuristic-based transductive learning to fill in missing\n",
    "        values in data matrix.\"\"\"\n",
    "        self.predict(train_data.copy())\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        raise NotImplementedError(\n",
    "            'baseline prediction not implemented for base class')\n",
    "\n",
    "    def rmse(self, test_data):\n",
    "        \"\"\"Calculate root mean squared error for predictions on test data.\"\"\"\n",
    "        return rmse(test_data, self.predicted)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return split_title(self.__class__.__name__)\n",
    "        \n",
    "\n",
    "\n",
    "# Implement the 3 baselines.\n",
    "\n",
    "class UniformRandomBaseline(Baseline):\n",
    "    \"\"\"Fill missing values with uniform random values.\"\"\"\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        nan_mask = np.isnan(train_data)\n",
    "        masked_train = np.ma.masked_array(train_data, nan_mask)\n",
    "        pmin, pmax = masked_train.min(), masked_train.max()\n",
    "        N = nan_mask.sum()\n",
    "        train_data[nan_mask] = np.random.uniform(pmin, pmax, N)\n",
    "        self.predicted = train_data\n",
    "\n",
    "\n",
    "class GlobalMeanBaseline(Baseline):\n",
    "    \"\"\"Fill in missing values using the global mean.\"\"\"\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        nan_mask = np.isnan(train_data)\n",
    "        train_data[nan_mask] = train_data[~nan_mask].mean()\n",
    "        self.predicted = train_data\n",
    "\n",
    "\n",
    "class MeanOfMeansBaseline(Baseline):\n",
    "    \"\"\"Fill in missing values using mean of user/item/global means.\"\"\"\n",
    "\n",
    "    def predict(self, train_data):\n",
    "        nan_mask = np.isnan(train_data)\n",
    "        masked_train = np.ma.masked_array(train_data, nan_mask)\n",
    "        global_mean = masked_train.mean()\n",
    "        user_means = masked_train.mean(axis=1)\n",
    "        item_means = masked_train.mean(axis=0)\n",
    "        self.predicted = train_data.copy()\n",
    "        n, m = train_data.shape\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                if np.ma.isMA(item_means[j]):\n",
    "                    self.predicted[i,j] = np.mean(\n",
    "                        (global_mean, user_means[i]))\n",
    "                else:\n",
    "                    self.predicted[i,j] = np.mean(\n",
    "                        (global_mean, user_means[i], item_means[j]))\n",
    "                    \n",
    "                    \n",
    "baseline_methods = OrderedDict()\n",
    "baseline_methods['ur'] = UniformRandomBaseline\n",
    "baseline_methods['gm'] = GlobalMeanBaseline\n",
    "baseline_methods['mom'] = MeanOfMeansBaseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Matrix Factorization\n",
    "\n",
    "[Probabilistic Matrix Factorization (PMF)](http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf) [3] is a probabilistic approach to the collaborative filtering problem that takes a Bayesian perspective. The ratings $R$ are modeled as draws from a Gaussian distribution.  The mean for $R_{ij}$ is $U_i V_j^T$. The precision $\\alpha$ is a fixed parameter that reflects the uncertainty of the estimations; the normal distribution is commonly reparameterized in terms of precision, which is the inverse of the variance. Complexity is controlled by placing zero-mean spherical Gaussian priors on $U$ and $V$. In other words, each row of $U$ is drawn from a multivariate Gaussian with mean $\\mu = 0$ and precision which is some multiple of the identity matrix $I$. Those multiples are $\\alpha_U$ for $U$ and $\\alpha_V$ for $V$. So our model is defined by:\n",
    "\n",
    "$\\newcommand\\given[1][]{\\:#1\\vert\\:}$\n",
    "\n",
    "\\begin{equation}\n",
    "P(R \\given U, V, \\alpha^2) = \n",
    "    \\prod_{i=1}^N \\prod_{j=1}^M\n",
    "        \\left[ \\mathcal{N}(R_{ij} \\given U_i V_j^T, \\alpha^{-1}) \\right]^{I_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(U \\given \\alpha_U^2) =\n",
    "    \\prod_{i=1}^N \\mathcal{N}(U_i \\given 0, \\alpha_U^{-1} \\boldsymbol{I})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(V \\given \\alpha_U^2) =\n",
    "    \\prod_{j=1}^M \\mathcal{N}(V_j \\given 0, \\alpha_V^{-1} \\boldsymbol{I})\n",
    "\\end{equation}\n",
    "\n",
    "Given small precision parameters, the priors on $U$ and $V$ ensure our latent variables do not grow too far from 0. This prevents overly strong user preferences and item factor compositions from being learned. This is commonly known as complexity control, where the complexity of the model here is measured by the magnitude of the latent variables. Controlling complexity like this helps prevent overfitting, which allows the model to generalize better for unseen data. We must also choose an appropriate $\\alpha$ value for the normal distribution for $R$. So the challenge becomes choosing appropriate values for $\\alpha_U$, $\\alpha_V$, and $\\alpha$. This challenge can be tackled with the soft weight-sharing methods discussed by [Nowland and Hinton, 1992](http://www.cs.toronto.edu/~fritz/absps/sunspots.pdf) [4]. However, for the purposes of this analysis, we will stick to using point estimates obtained from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "# Enable on-the-fly graph computations, but ignore \n",
    "# absence of intermediate test values.\n",
    "theano.config.compute_test_value = 'ignore'\n",
    "\n",
    "# Set up logging.\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "class PMF(object):\n",
    "    \"\"\"Probabilistic Matrix Factorization model using pymc3.\"\"\"\n",
    "\n",
    "    def __init__(self, train, dim, alpha=2, std=0.01, bounds=(-10, 10)):\n",
    "        \"\"\"Build the Probabilistic Matrix Factorization model using pymc3.\n",
    "\n",
    "        :param np.ndarray train: The training data to use for learning the model.\n",
    "        :param int dim: Dimensionality of the model; number of latent factors.\n",
    "        :param int alpha: Fixed precision for the likelihood function.\n",
    "        :param float std: Amount of noise to use for model initialization.\n",
    "        :param (tuple of int) bounds: (lower, upper) bound of ratings.\n",
    "            These bounds will simply be used to cap the estimates produced for R.\n",
    "\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.alpha = alpha\n",
    "        self.std = np.sqrt(1.0 / alpha)\n",
    "        self.bounds = bounds\n",
    "        self.data = train.copy()\n",
    "        n, m = self.data.shape\n",
    "\n",
    "        # Perform mean value imputation\n",
    "        nan_mask = np.isnan(self.data)\n",
    "        self.data[nan_mask] = self.data[~nan_mask].mean()\n",
    "\n",
    "        # Low precision reflects uncertainty; prevents overfitting.\n",
    "        # Set to the mean variance across users and items.\n",
    "        self.alpha_u = 1 / self.data.var(axis=1).mean()\n",
    "        self.alpha_v = 1 / self.data.var(axis=0).mean()\n",
    "\n",
    "        # Specify the model.\n",
    "        logging.info('building the PMF model')\n",
    "        with pm.Model() as pmf:\n",
    "            U = pm.MvNormal(\n",
    "                'U', mu=0, tau=self.alpha_u * np.eye(dim),\n",
    "                shape=(n, dim), testval=np.random.randn(n, dim) * std)\n",
    "            V = pm.MvNormal(\n",
    "                'V', mu=0, tau=self.alpha_v * np.eye(dim),\n",
    "                shape=(m, dim), testval=np.random.randn(m, dim) * std)\n",
    "            R = pm.Normal(\n",
    "                'R', mu=theano.tensor.dot(U, V.T), tau=self.alpha * np.ones((n, m)),\n",
    "                observed=self.data)\n",
    "\n",
    "        logging.info('done building the PMF model') \n",
    "        self.model = pmf\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need functions for calculating the MAP and performing sampling on our PMF model. When the observation noise variance $\\alpha$ and the prior variances $\\alpha_U$ and $\\alpha_V$ are all kept fixed, maximizing the log posterior is equivalent to minimizing the sum-of-squared-errors objective function with quadratic regularization terms.\n",
    "\n",
    "$$ E = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} (R_{ij} - U_i V_j^T)^2 + \\frac{\\lambda_U}{2} \\sum_{i=1}^N \\|U\\|_{Fro}^2 + \\frac{\\lambda_V}{2} \\sum_{j=1}^M \\|V\\|_{Fro}^2, $$\n",
    "\n",
    "where $\\lambda_U = \\alpha_U / \\alpha$, $\\lambda_V = \\alpha_V / \\alpha$, and $\\|\\cdot\\|_{Fro}^2$ denotes the Frobenius norm [3]. Minimizing this objective function gives a local minimum, which is essentially a maximum a posteriori (MAP) estimate. While it is possible to use a fast Stochastic Gradient Descent procedure to find this MAP, we'll be finding it using the utilities built into `pymc3`. In particular, we'll use `find_MAP` with Powell optimization (`scipy.optimize.fmin_powell`). Having found this MAP estimate, we can use it as our starting point for MCMC sampling.\n",
    "\n",
    "Since it is a reasonably complex model, we expect the MAP estimation to take some time. So let's save it after we've found it. Note that we define a function for finding the MAP below, assuming it will receive a namespace with some variables in it. Then we attach that function to the PMF class, where it will have such a namespace after initialization. The PMF class is defined in pieces this way so I can say a few things between each piece to make it clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import ujson as json\n",
    "except ImportError:\n",
    "    import json\n",
    "\n",
    "\n",
    "# First define functions to save our MAP estimate after it is found.\n",
    "# We adapt these from `pymc3`'s `backends` module, where the original\n",
    "# code is used to save the traces from MCMC samples.\n",
    "def save_np_vars(vars, savedir):\n",
    "    \"\"\"Save a dictionary of numpy variables to `savedir`. We assume\n",
    "    the directory does not exist; an OSError will be raised if it does.\n",
    "    \"\"\"\n",
    "    logging.info('writing numpy vars to directory: %s' % savedir)\n",
    "    if not os.path.isdir(savedir):\n",
    "        os.mkdir(savedir)\n",
    "    shapes = {}\n",
    "    for varname in vars:\n",
    "        data = vars[varname]\n",
    "        var_file = os.path.join(savedir, varname + '.txt')\n",
    "        np.savetxt(var_file, data.reshape(-1, data.size))\n",
    "        shapes[varname] = data.shape\n",
    "\n",
    "        ## Store shape information for reloading.\n",
    "        shape_file = os.path.join(savedir, 'shapes.json')\n",
    "        with open(shape_file, 'w') as sfh:\n",
    "            json.dump(shapes, sfh)\n",
    "            \n",
    "            \n",
    "def load_np_vars(savedir):\n",
    "    \"\"\"Load numpy variables saved with `save_np_vars`.\"\"\"\n",
    "    shape_file = os.path.join(savedir, 'shapes.json')\n",
    "    with open(shape_file, 'r') as sfh:\n",
    "        shapes = json.load(sfh)\n",
    "\n",
    "    vars = {}\n",
    "    for varname, shape in shapes.items():\n",
    "        var_file = os.path.join(savedir, varname + '.txt')\n",
    "        vars[varname] = np.loadtxt(var_file).reshape(shape)\n",
    "        \n",
    "    return vars\n",
    "\n",
    "\n",
    "# Now define the MAP estimation infrastructure.\n",
    "def _map_dir(self):\n",
    "    basename = 'pmf-map-d%d' % self.dim\n",
    "    return os.path.join(DATA_DIR, basename)\n",
    "\n",
    "def _find_map(self):\n",
    "    \"\"\"Find mode of posterior using Powell optimization.\"\"\"\n",
    "    tstart = time.time()\n",
    "    with self.model:\n",
    "        logging.info('finding PMF MAP using Powell optimization...')\n",
    "        self._map = pm.find_MAP(fmin=sp.optimize.fmin_powell, disp=True)\n",
    "\n",
    "    elapsed = int(time.time() - tstart)\n",
    "    logging.info('found PMF MAP in %d seconds' % elapsed)\n",
    "    \n",
    "    # This is going to take a good deal of time to find, so let's save it.\n",
    "    save_np_vars(self._map, self.map_dir)\n",
    "    \n",
    "def _load_map(self):\n",
    "    self._map = load_np_vars(self.map_dir)\n",
    "\n",
    "def _map(self):\n",
    "    try:\n",
    "        return self._map\n",
    "    except:\n",
    "        if os.path.isdir(self.map_dir):\n",
    "            self.load_map()\n",
    "        else:\n",
    "            self.find_map()\n",
    "        return self._map\n",
    "\n",
    "    \n",
    "# Update our class with the new MAP infrastructure.\n",
    "PMF.find_map = _find_map\n",
    "PMF.load_map = _load_map\n",
    "PMF.map_dir = property(_map_dir)\n",
    "PMF.map = property(_map)\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now our PMF class has a `map` `property` which will either be found using Powell optimization or loaded from a previous optimization. Once we have the MAP, we can use it as a starting point for our MCMC sampler. We'll need a sampling function in order to draw MCMC samples to approximate the posterior distribution of the PMF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "# Draw MCMC samples.\n",
    "def _trace_dir(self):\n",
    "    basename = 'pmf-mcmc-d%d' % self.dim\n",
    "    return os.path.join(DATA_DIR, basename)\n",
    "\n",
    "def _draw_samples(self, nsamples=1000, njobs=2):\n",
    "    # First make sure the trace_dir does not already exist.\n",
    "    if os.path.isdir(self.trace_dir):\n",
    "        shutil.rmtree(self.trace_dir)\n",
    "\n",
    "    with self.model:\n",
    "        logging.info('drawing %d samples using %d jobs' % (nsamples, njobs))\n",
    "        backend = pm.backends.Text(self.trace_dir)\n",
    "        logging.info('backing up trace to directory: %s' % self.trace_dir)\n",
    "        self.trace = pm.sample(draws=nsamples, init='advi',\n",
    "                               n_init=150000, njobs=njobs, trace=backend)\n",
    "        \n",
    "def _load_trace(self):\n",
    "    with self.model:\n",
    "        self.trace = pm.backends.text.load(self.trace_dir)\n",
    "\n",
    "        \n",
    "# Update our class with the sampling infrastructure.\n",
    "PMF.trace_dir = property(_trace_dir)\n",
    "PMF.draw_samples = _draw_samples\n",
    "PMF.load_trace = _load_trace\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could define some kind of default trace property like we did for the MAP, but that would mean using possibly nonsensical values for `nsamples` and `njobs`. Better to leave it as a non-optional call to `draw_samples`. Finally, we'll need a function to make predictions using our inferred values for $U$ and $V$. For user $i$ and joke $j$, a prediction is generated by drawing from $\\mathcal{N}(U_i V_j^T, \\alpha)$. To generate predictions from the sampler, we generate an $R$ matrix for each $U$ and $V$ sampled, then we combine these by averaging over the $K$ samples.\n",
    "\n",
    "\\begin{equation}\n",
    "P(R_{ij}^* \\given R, \\alpha, \\alpha_U, \\alpha_V) \\approx\n",
    "    \\frac{1}{K} \\sum_{k=1}^K \\mathcal{N}(U_i V_j^T, \\alpha)\n",
    "\\end{equation}\n",
    "\n",
    "We'll want to inspect the individual $R$ matrices before averaging them for diagnostic purposes. So we'll write code for the averaging piece during evaluation. The function below simply draws an $R$ matrix given a $U$ and $V$ and the fixed $\\alpha$ stored in the PMF object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _predict(self, U, V):\n",
    "    \"\"\"Estimate R from the given values of U and V.\"\"\"\n",
    "    R = np.dot(U, V.T)\n",
    "    n, m = R.shape\n",
    "    sample_R = np.array([\n",
    "        [np.random.normal(R[i,j], self.std) for j in range(m)]\n",
    "        for i in range(n)\n",
    "    ])\n",
    "\n",
    "    # bound ratings\n",
    "    low, high = self.bounds\n",
    "    sample_R[sample_R < low] = low\n",
    "    sample_R[sample_R > high] = high\n",
    "    return sample_R\n",
    "\n",
    "\n",
    "PMF.predict = _predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final thing to note: the dot products in this model are often constrained using a logistic function $g(x) = 1/(1 + exp(-x))$, that bounds the predictions to the range [0, 1]. To facilitate this bounding, the ratings are also mapped to the range [0, 1] using $t(x) = (x + min) / range$. The authors of PMF also introduced a constrained version which performs better on users with less ratings [3]. Both models are generally improvements upon the basic model presented here. However, in the interest of time and space, these will not be implemented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Metrics\n",
    "\n",
    "In order to understand how effective our models are, we'll need to be able to evaluate them. We'll be evaluating in terms of root mean squared error (RMSE), which looks like this:\n",
    "\n",
    "\\begin{equation}\n",
    "RMSE = \\sqrt{ \\frac{ \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} (R_{ij} - R_{ij}^*)^2 }\n",
    "                   { \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} } }\n",
    "\\end{equation}\n",
    "\n",
    "In this case, the RMSE can be thought of as the standard deviation of our predictions from the actual user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define our evaluation function.\n",
    "def rmse(test_data, predicted):\n",
    "    \"\"\"Calculate root mean squared error.\n",
    "    Ignoring missing values in the test data.\n",
    "    \"\"\"\n",
    "    I = ~np.isnan(test_data)   # indicator for missing values\n",
    "    N = I.sum()                # number of non-missing values\n",
    "    sqerror = abs(test_data - predicted) ** 2  # squared error array\n",
    "    mse = sqerror[I].sum() / N                 # mean squared error\n",
    "    return np.sqrt(mse)                        # RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data vs. Test Data\n",
    "\n",
    "The next thing we need to do is split our data into a training set and a test set. Matrix factorization techniques use [transductive learning](http://en.wikipedia.org/wiki/Transduction_%28machine_learning%29) rather than inductive learning. So we produce a test set by taking a random sample of the cells in the full $N \\times M$ data matrix. The values selected as test samples are replaced with `nan` values in a copy of the original data matrix to produce the training set. Since we'll be producing random splits, let's also write out the train/test sets generated. This will allow us to replicate our results. We'd like to be able to idenfity which split is which, so we'll take a hash of the indices selected for testing and use that to save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unicode-objects must be encoded before hashing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-72719e567c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-72719e567c08>\u001b[0m in \u001b[0;36msplit_train_test\u001b[0;34m(data, percent_test)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Finally, hash the indices and save the train/test sets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mindex_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0msavedir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0msave_np_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unicode-objects must be encoded before hashing"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "# Define a function for splitting train/test data.\n",
    "def split_train_test(data, percent_test=10):\n",
    "    \"\"\"Split the data into train/test sets.\n",
    "    :param int percent_test: Percentage of data to use for testing. Default 10.\n",
    "    \"\"\"\n",
    "    n, m = data.shape             # # users, # jokes\n",
    "    N = n * m                     # # cells in matrix\n",
    "    test_size = N / percent_test  # use 10% of data as test set\n",
    "    train_size = N - test_size    # and remainder for training\n",
    "\n",
    "    # Prepare train/test ndarrays.\n",
    "    train = data.copy().values\n",
    "    test = np.ones(data.shape) * np.nan\n",
    "\n",
    "    # Draw random sample of training data to use for testing.\n",
    "    tosample = np.where(~np.isnan(train))       # ignore nan values in data\n",
    "    \n",
    "    #idx_pairs = zip(tosample[0], tosample[1])   # tuples of row/col index pairs\n",
    "    idx_pairs = list(zip(tosample[0], tosample[1]))   # tuples of row/col index pairs\n",
    "    \n",
    "    indices = np.arange(len(idx_pairs))         # indices of index pairs\n",
    "    sample = np.random.choice(indices, replace=False, size=int(test_size))\n",
    "\n",
    "    # Transfer random sample from train set to test set.\n",
    "    for idx in sample:\n",
    "        idx_pair = idx_pairs[idx]\n",
    "        test[idx_pair] = train[idx_pair]  # transfer to test set\n",
    "        train[idx_pair] = np.nan          # remove from train set\n",
    "\n",
    "    # Verify everything worked properly\n",
    "    assert(np.isnan(train).sum() == test_size)\n",
    "    assert(np.isnan(test).sum() == train_size)\n",
    "    \n",
    "    # Finally, hash the indices and save the train/test sets.\n",
    "    index_string = ''.join(map(str, np.sort(sample)))\n",
    "    name = hashlib.sha1(index_string).hexdigest()\n",
    "    savedir = os.path.join(DATA_DIR, name)\n",
    "    save_np_vars({'train': train, 'test': test}, savedir)\n",
    "    \n",
    "    # Return train set, test set, and unique hash of indices.\n",
    "    return train, test, name\n",
    "\n",
    "\n",
    "def load_train_test(name):\n",
    "    \"\"\"Load the train/test sets.\"\"\"\n",
    "    savedir = os.path.join(DATA_DIR, name)\n",
    "    vars = load_np_vars(savedir)\n",
    "    return vars['train'], vars['test']\n",
    "\n",
    "train, test, name = split_train_test(data)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate reproducibility, I've produced a train/test split using the code above which we'll now use for all the evaluations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/2f7be834c19f630bc89d550e41485a5a54d28af7/shapes.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-137857e20ef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2f7be834c19f630bc89d550e41485a5a54d28af7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-926e65e1ad4d>\u001b[0m in \u001b[0;36mload_train_test\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;34m\"\"\"Load the train/test sets.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0msavedir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mvars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_np_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavedir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e7dc6a5a70d6>\u001b[0m in \u001b[0;36mload_np_vars\u001b[0;34m(savedir)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m\"\"\"Load numpy variables saved with `save_np_vars`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mshape_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavedir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shapes.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/2f7be834c19f630bc89d550e41485a5a54d28af7/shapes.json'"
     ]
    }
   ],
   "source": [
    "train, test = load_train_test('2f7be834c19f630bc89d550e41485a5a54d28af7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's see the results:\n",
    "baselines = {}\n",
    "for name in baseline_methods:\n",
    "    Method = baseline_methods[name]\n",
    "    method = Method(train)\n",
    "    baselines[name] = method.rmse(test)\n",
    "    print('%s RMSE:\\t%.5f' % (method, baselines[name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected: the uniform random baseline is the worst by far, the global mean baseline is next best, and the mean of means method is our best baseline. Now let's see how PMF stacks up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use a fixed precision for the likelihood.\n",
    "# This reflects uncertainty in the dot product.\n",
    "# We choose 2 in the footsteps Salakhutdinov\n",
    "# Mnihof.\n",
    "ALPHA = 2\n",
    "\n",
    "# The dimensionality D; the number of latent factors.\n",
    "# We can adjust this higher to try to capture more subtle\n",
    "# characteristics of each joke. However, the higher it is,\n",
    "# the more expensive our inference procedures will be.\n",
    "# Specifically, we have D(N + M) latent variables. For our\n",
    "# Jester dataset, this means we have D(1100), so for 5\n",
    "# dimensions, we are sampling 5500 latent variables.\n",
    "DIM = 5\n",
    "\n",
    "\n",
    "pmf = PMF(train, DIM, ALPHA, std=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions Using MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find MAP for PMF.\n",
    "pmf.find_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. The first thing we want to do is make sure the MAP estimate we obtained is reasonable. We can do this by computing RMSE on the predicted ratings obtained from the MAP values of $U$ and $V$. First we define a function for generating the predicted ratings $R$ from $U$ and $V$. We ensure the actual rating bounds are enforced by setting all values below -10 to -10 and all values above 10 to 10. Finally, we compute RMSE for both the training set and the test set. We expect the test RMSE to be higher. The difference between the two gives some idea of how much we have overfit. Some difference is always expected, but a very low RMSE on the training set with a high RMSE on the test set is a definite sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_map(pmf_model, train, test):\n",
    "    U = pmf_model.map['U']\n",
    "    V = pmf_model.map['V']\n",
    "    \n",
    "    # Make predictions and calculate RMSE on train & test sets.\n",
    "    predictions = pmf_model.predict(U, V)\n",
    "    train_rmse = rmse(train, predictions)\n",
    "    test_rmse = rmse(test, predictions)\n",
    "    overfit = test_rmse - train_rmse\n",
    "    \n",
    "    # Print report.\n",
    "    print('PMF MAP training RMSE: %.5f' % train_rmse)\n",
    "    print('PMF MAP testing RMSE:  %.5f' % test_rmse)\n",
    "    print('Train/test difference: %.5f' % overfit)\n",
    "    \n",
    "    return test_rmse\n",
    "    \n",
    "\n",
    "# Add eval function to PMF class.\n",
    "PMF.eval_map = eval_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate PMF MAP estimates.\n",
    "pmf_map_rmse = pmf.eval_map(train, test)\n",
    "pmf_improvement = baselines['mom'] - pmf_map_rmse\n",
    "print('PMF MAP Improvement:   %.5f' % pmf_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see a pretty nice improvement here when compared to our best baseline, which was the mean of means method. We also have a fairly small difference in the RMSE values between the train and the test sets. This indicates that the point estimates for $\\alpha_U$ and $\\alpha_V$ that we calculated from our data are doing a good job of controlling model complexity. Now let's see if we can improve our estimates by approximating our posterior distribution with MCMC sampling. We'll draw 1000 samples and back them up using the `pymc3.backend.Text` backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions using MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Draw MCMC samples.\n",
    "pmf.draw_samples(300)\n",
    "\n",
    "# uncomment to load previous trace rather than drawing new samples.\n",
    "# pmf.load_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics and Posterior Predictive Check\n",
    "\n",
    "The next step is to check how many samples we should discard as burn-in. Normally, we'd do this using a traceplot to get some idea of where the sampled variables start to converge. In this case, we have high-dimensional samples, so we need to find a way to approximate them. One way was proposed by [Salakhutdinov and Mnih, p.886](https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf). We can calculate the Frobenius norms of $U$ and $V$ at each step and monitor those for convergence. This essentially gives us some idea when the average magnitude of the latent variables is stabilizing. The equations for the Frobenius norms of $U$ and $V$ are shown below. We will use `numpy`'s `linalg` package to calculate these.\n",
    "\n",
    "$$ \\|U\\|_{Fro}^2 = \\sqrt{\\sum_{i=1}^N \\sum_{d=1}^D |U_{id}|^2}, \\hspace{40pt} \\|V\\|_{Fro}^2 = \\sqrt{\\sum_{j=1}^M \\sum_{d=1}^D |V_{jd}|^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _norms(pmf_model, monitor=('U', 'V'), ord='fro'):\n",
    "    \"\"\"Return norms of latent variables at each step in the\n",
    "    sample trace. These can be used to monitor convergence\n",
    "    of the sampler.\n",
    "    \"\"\"\n",
    "    monitor = ('U', 'V')\n",
    "    norms = {var: [] for var in monitor}\n",
    "    for sample in pmf_model.trace:\n",
    "        for var in monitor:\n",
    "            norms[var].append(np.linalg.norm(sample[var], ord))\n",
    "    return norms\n",
    "\n",
    "\n",
    "def _traceplot(pmf_model):\n",
    "    \"\"\"Plot Frobenius norms of U and V as a function of sample #.\"\"\"\n",
    "    trace_norms = pmf_model.norms()\n",
    "    u_series = pd.Series(trace_norms['U'])\n",
    "    v_series = pd.Series(trace_norms['V'])\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    u_series.plot(kind='line', ax=ax1, grid=False,\n",
    "                  title=\"$\\|U\\|_{Fro}^2$ at Each Sample\")\n",
    "    v_series.plot(kind='line', ax=ax2, grid=False,\n",
    "                  title=\"$\\|V\\|_{Fro}^2$ at Each Sample\")\n",
    "    ax1.set_xlabel(\"Sample Number\")\n",
    "    ax2.set_xlabel(\"Sample Number\")\n",
    "    \n",
    "    \n",
    "PMF.norms = _norms\n",
    "PMF.traceplot = _traceplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pmf.traceplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we get convergence of $U$ and $V$ after about 200 samples. When testing for convergence, we also want to see convergence of the particular statistics we are looking for, since different characteristics of the posterior may converge at different rates. Let's also do a traceplot of the RSME. We'll compute RMSE for both the train and the test set, even though the convergence is indicated by RMSE on the training set alone. In addition, let's compute a running RMSE on the train/test sets to see how aggregate performance improves or decreases as we continue to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _running_rmse(pmf_model, test_data, train_data, burn_in=0, plot=True):\n",
    "    \"\"\"Calculate RMSE for each step of the trace to monitor convergence.\n",
    "    \"\"\"\n",
    "    burn_in = burn_in if len(pmf_model.trace) >= burn_in else 0\n",
    "    results = {'per-step-train': [], 'running-train': [],\n",
    "               'per-step-test': [], 'running-test': []}\n",
    "    R = np.zeros(test_data.shape)\n",
    "    for cnt, sample in enumerate(pmf_model.trace[burn_in:]):\n",
    "        sample_R = pmf_model.predict(sample['U'], sample['V'])\n",
    "        R += sample_R\n",
    "        running_R = R / (cnt + 1)\n",
    "        results['per-step-train'].append(rmse(train_data, sample_R))\n",
    "        results['running-train'].append(rmse(train_data, running_R))\n",
    "        results['per-step-test'].append(rmse(test_data, sample_R))\n",
    "        results['running-test'].append(rmse(test_data, running_R))\n",
    "    \n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    if plot:\n",
    "        results.plot(\n",
    "            kind='line', grid=False, figsize=(15, 7),\n",
    "            title='Per-step and Running RMSE From Posterior Predictive')\n",
    "        \n",
    "    # Return the final predictions, and the RMSE calculations\n",
    "    return running_R, results\n",
    "\n",
    "\n",
    "PMF.running_rmse = _running_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted, results = pmf.running_rmse(test, train, burn_in=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And our final RMSE?\n",
    "final_test_rmse = results['running-test'].values[-1]\n",
    "final_train_rmse = results['running-train'].values[-1]\n",
    "print('Posterior predictive train RMSE: %.5f' % final_train_rmse)\n",
    "print('Posterior predictive test RMSE:  %.5f' % final_test_rmse)\n",
    "print('Train/test difference:           %.5f' % (final_test_rmse - final_train_rmse))\n",
    "print('Improvement from MAP:            %.5f' % (pmf_map_rmse - final_test_rmse))\n",
    "print('Improvement from Mean of Means:  %.5f' % (baselines['mom'] - final_test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some interesting results here. As expected, our MCMC sampler provides lower error on the training set. However, it seems it does so at the cost of overfitting the data. This results in a decrease in test RMSE as compared to the MAP, even though it is still much better than our best baseline. So why might this be the case? Recall that we used point estimates for our precision paremeters $\\alpha_U$ and $\\alpha_V$ and we chose a fixed precision $\\alpha$. It is quite likely that by doing this, we constrained our posterior in a way that biased it towards the training data. In reality, the variance in the user ratings and the joke ratings is unlikely to be equal to the means of sample variances we used. Also, the most reasonable observation precision $\\alpha$ is likely different as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "Let's summarize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = 100  # RMSE doesn't really change after 100th sample anyway.\n",
    "all_results = pd.DataFrame({\n",
    "    'uniform random': np.repeat(baselines['ur'], size),\n",
    "    'global means': np.repeat(baselines['gm'], size),\n",
    "    'mean of means': np.repeat(baselines['mom'], size),\n",
    "    'PMF MAP': np.repeat(pmf_map_rmse, size),\n",
    "    'PMF MCMC': results['running-test'][:size],\n",
    "})\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "all_results.plot(kind='line', grid=False, ax=ax,\n",
    "                 title='RMSE for all methods')\n",
    "ax.set_xlabel(\"Number of Samples\")\n",
    "ax.set_ylabel(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We set out to predict user preferences for unseen jokes. First we discussed the intuitive notion behind the user-user and item-item neighborhood approaches to collaborative filtering. Then we formalized our intuitions. With a firm understanding of our problem context, we moved on to exploring our subset of the Jester data. After discovering some general patterns, we defined three baseline methods: uniform random, global mean, and mean of means. With the goal of besting our baseline methods, we implemented the basic version of Probabilistic Matrix Factorization (PMF) using `pymc3`.\n",
    "\n",
    "Our results demonstrate that the mean of means method is our best baseline on our prediction task. As expected, we are able to obtain a significant decrease in RMSE using the PMF MAP estimate obtained via Powell optimization. We illustrated one way to monitor convergence of an MCMC sampler with a high-dimensionality sampling space using the Frobenius norms of the sampled variables. The traceplots using this method seem to indicate that our sampler converged to the posterior. Results using this posterior showed that attempting to improve the MAP estimation using MCMC sampling actually overfit the training data and increased test RMSE. This was likely caused by the constraining of the posterior via fixed precision parameters $\\alpha$, $\\alpha_U$, and $\\alpha_V$.\n",
    "\n",
    "As a followup to this analysis, it would be interesting to also implement the logistic and constrained versions of PMF. We expect both models to outperform the basic PMF model. We could also implement the [fully Bayesian version of PMF](https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf) (BPMF), which places hyperpriors on the model parameters to automatically learn ideal mean and precision parameters for $U$ and $V$. This would likely resolve the issue we faced in this analysis. We would expect BPMF to improve upon the MAP estimation produced here by learning more suitable hyperparameters and parameters. For a basic (but working!) implementation of BPMF in `pymc3`, see [this gist](https://gist.github.com/macks22/00a17b1d374dfc267a9a).\n",
    "\n",
    "If you made it this far, then congratulations! You now have some idea of how to build a basic recommender system. These same ideas and methods can be used on many different recommendation tasks. Items can be movies, products, advertisements, courses, or even other people. Any time you can build yourself a user-item matrix with user preferences in the cells, you can use these types of collaborative filtering algorithms to predict the missing values. If you want to learn more about recommender systems, the first reference is a good place to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1.  Y. Koren, R. Bell, and C. Volinsky, “Matrix Factorization Techniques for Recommender Systems,” Computer, vol. 42, no. 8, pp. 30–37, Aug. 2009.\n",
    "2.  K. Goldberg, T. Roeder, D. Gupta, and C. Perkins, “Eigentaste: A constant time collaborative filtering algorithm,” Information Retrieval, vol. 4, no. 2, pp. 133–151, 2001.\n",
    "3.  A. Mnih and R. Salakhutdinov, “Probabilistic matrix factorization,” in Advances in neural information processing systems, 2007, pp. 1257–1264.\n",
    "4.  S. J. Nowlan and G. E. Hinton, “Simplifying Neural Networks by Soft Weight-sharing,” Neural Comput., vol. 4, no. 4, pp. 473–493, Jul. 1992.\n",
    "5.  R. Salakhutdinov and A. Mnih, “Bayesian Probabilistic Matrix Factorization Using Markov Chain Monte Carlo,” in Proceedings of the 25th International Conference on Machine Learning, New York, NY, USA, 2008, pp. 880–887.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
