{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Why Bayesian Statistics?</h1>\n",
    "We've discussed MLE at length at this point in the course.  We state $P(\\theta | x_{1, ..., n}) = \\mathcal{L}(x_{1, ..., n}|\\theta)$, and we then make modifications from here.  This of course is a simplification of Bayes Theorem, and it has been the basis of most of the statistics and Machine Learning algorithms.  Frequentist approaches are very popular, as they offer a systematic an widely accepted approach to scientific inquiry.  Bayesian statistics have certain strengths that are worth considering:\n",
    "<ul>\n",
    "    <li>Bayesian statistics reflects many peoples intuition for probabilities</li>\n",
    "    <li>Bayesian statistics offers a mechanism for updating beliefs in an iterative manner</li>\n",
    "    <li>Bayesian statistics offers a mechanism for transferring domain knowledge across problems</li>\n",
    "</ul>\n",
    "\n",
    "<h2>So what's the Difference?</h2>\n",
    "Let's start by looking at the elements of Bayes theorem, in particular when we consider it's application to identifying a set of parameters.  The equation is:\n",
    "$$\n",
    "p(\\theta | x_{1, ..., n}) = \\frac{\\mathcal{L}(x_{1, ..., n}|\\theta)*p(\\theta)}{p(x)}\n",
    "$$\n",
    "\n",
    "<h3>Posterior Probabilities</h3>\n",
    "While the equation is familiar, we need to remind ourselves what the equation means.  In MLE, we simply treated parameter inference as a maximization problem.  We aimed to find a single value for a parameter $\\theta$ that Maximized our probability.  We can do the same thing using the Bayesian method, however, in practice we tend to care more about the actual probabilities of parameters.  In this way, the goal of Bayesian inference is finding $p(\\theta | x_{1, ..., n})$ the <b>posterior probability</b> that a set of parameters is valid.\n",
    "\n",
    "<h3>Prior Probabilities</h3>\n",
    "Our <b>likelihood function</b>, $\\mathcal{L}(x_{1, ..., n}|\\theta)$, takes the same role as it had in MLE.  Recall, in our derivations of MLE, we stated that Frequentists have the assumption that all outcomes are equally probable.  We then don't incorporate <b>prior probabilities</b>, $p(\\theta)$.  Bayesians don't have this assumption, and is perhaps the greatest difference between the two approaches.\n",
    "\n",
    "<h3>Marginal Likelihood</h3>\n",
    "In MLE (and in fact, MAP), we only care about finding the value of $\\theta$ that is most Likely.  In order for the calculation to actually result in an interpretable probability, we need the normalization constant, $p(x)$.  We call this normalization the <b>marginal likelihood</b>.\n",
    "\n",
    "<h2>Example - The Cheating Gambler</h2>\n",
    "Suppose you're brother has a loaded coin.  You know from experience that this coin has the probability of coming up heads, $p(x=1) = .7$ (we do not consider this value under question).  We see him gambling with a friend, and we are unsure if he is using the loaded coin, or a fair coin.  Before seeing any flips, we think there is a 60% probability he is using a loaded coin.  We observe him flip a coin 5 times, which results in 2 heads.\n",
    "<br/><br/>\n",
    "<b>Questions:</b>\n",
    "<ol>\n",
    "    <li>What are our prior and posterior probabilities describing?</li>\n",
    "    <li>What are the remaining parts of our update formula?</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior loaded: 0.388, posterior fair: 0.612\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "\n",
    "def update_loaded_coin_probabilities(n_trial, n_success, prior_loaded, p_heads_loaded):\n",
    "    prior_fair= 1-prior_loaded\n",
    "    likelihood_fair, likelihood_loaded = binom.pmf(n_success, n_trial, .5), binom.pmf(n_success, n_trial, p_heads_loaded)\n",
    "    marginal_likelihood = prior_fair*likelihood_fair+prior_loaded*likelihood_loaded\n",
    "    posterior_fair = prior_fair*likelihood_fair/marginal_likelihood\n",
    "    posterior_loaded = prior_loaded*likelihood_loaded/marginal_likelihood\n",
    "    return posterior_loaded, posterior_fair\n",
    "\n",
    "posteriors = update_loaded_coin_probabilities(5, 2, .6, .7)\n",
    "print(\"Posterior loaded: {0:.3f}, posterior fair: {1:.3f}\".format(posteriors[0], posteriors[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about this.  We came into the scenario thinking that it was more probable that the coin was loaded than not. The evidence did not seem to support this belief, and the update mechanic reflects this.\n",
    "\n",
    "<h3>Follow Up Questions</h3>\n",
    "For the following questions, start by considering what your intuition is for the following scenarios.  Use the above function to calculate only after you've made a guess. \n",
    "<ol>\n",
    "<li>What happens if our initial beliefs are stronger (e.g. our initial $p_{loaded}=.9$)?</li>\n",
    "<li>What if we start with even initial probability (e.g. $p_{loaded}$=.5)?</li>\n",
    "<li>What if our initial priors are the same, but the amount of data doubles (i.e. 4 heads in 10 flips)?</li>\n",
    "</ol>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7921101649813731, 0.20788983501862687)\n",
      "(0.29743705035971224, 0.7025629496402878)\n",
      "(0.21188510948386136, 0.7881148905161387)\n"
     ]
    }
   ],
   "source": [
    "# Run your tests here\n",
    "print(update_loaded_coin_probabilities(5, 2, .9, .7))\n",
    "print(update_loaded_coin_probabilities(5, 2, .5, .7))\n",
    "print(update_loaded_coin_probabilities(10, 4, .6, .7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Update Mechanic</h2>\n",
    "Another feature of Baysian statistics that distinguishes it from the Frequentist approach is the concept of the update mechanic.  Before we have any observed data, we have the probability that the coin is loaded or fair.  After we observe data, we have the same thing!  How do we use these probabilities?  Well first, we are going to interact with the world with the \"degree of belief\" according to the evidence.  In the original scenario, this means it's about twice as likely that the coin is fair as loaded.  What would happen if we see a second set of five flips, and we again see 2 out of 5 flip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior loaded: 0.212, posterior fair: 0.788\n"
     ]
    }
   ],
   "source": [
    "posteriors = update_loaded_coin_probabilities(5, 2, 0.388, .7)\n",
    "print(\"Posterior loaded: {0:.3f}, posterior fair: {1:.3f}\".format(posteriors[0], posteriors[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply use the posterior probability from our initial update as the prior probability in the next step!  This is why Bayesian inference is sometimes called \"Bayesian Updates\".  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
