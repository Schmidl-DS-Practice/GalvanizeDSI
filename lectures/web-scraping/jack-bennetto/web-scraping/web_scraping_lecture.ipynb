{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping, HTML and Beautiful Soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Objectives:\n",
    "* Describe a typical web scraping data pipeline.\n",
    "* Explain the basic concepts of HTML.\n",
    "* Write code to pull elements from a web page using BeautifulSoup.\n",
    "* Use an existing API to fetch data and parse using BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [w3 schools](http://www.w3schools.com/) : HTML tags and their attributes.\n",
    "* [BeautifulSoup Documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [Scrape anonymously with Tor](https://deshmukhsuraj.wordpress.com/2015/03/08/anonymous-web-scraping-using-python-and-tor/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**H**yper**T**ext **M**arkup **L**anguage\n",
    "\n",
    "A *markup language* (think markdown) that forms the building blocks of all websites. Hypertext is text that includes links to other pages. HTML specifies not just the text of the document and the links but also the organization (into sections and paragraphs and lists and such). It can also control the layout of the document (the font and color and size and such) though that is properly handled with Cascading Style Sheets (CSS). \n",
    "\n",
    "It consists of opening and closing tags enclosed in angle brackets (like `<html>` and `</html>`) often with more HTML in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minimal HTML document, unfortuantely, contains a lot of cruft.  Here's one I got from [https://www.sitepoint.com/a-minimal-html-document/](https://www.sitepoint.com/a-minimal-html-document/).\n",
    "\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
    "    \"http://www.w3.org/TR/html4/strict.dtd\">\n",
    "<html lang=\"en\">\n",
    "  <head>\n",
    "  \n",
    "    <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\">\n",
    "    <title>title</title>\n",
    "    <link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\">\n",
    "    <script type=\"text/javascript\" src=\"script.js\"></script>\n",
    "  </head>\n",
    "  <body>\n",
    "\t\t\n",
    "  </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key=value pairs inside of a tag are called attributes. The `<link>` and `<script>` tags aren't necessary, but appear in more or less every HTML document.\n",
    "\n",
    "* The `<link>` tag points to a **stylesheet**, which controls who different parts of the docuemnt are rendered in the browser.  This makes things pretty.\n",
    "* The `<script>` tag points to a **javascript** program.  This allows programmers to add *dynamic behaviour* to a html document.\n",
    "* The `<body>` tag contains the guts of your document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Tags\n",
    "\n",
    "```html\n",
    "<a href=\"http://www.w3schools.com\">A hyperlink to W3Schools.com!</a>\n",
    "\n",
    "<h1>This is a header!</h1>\n",
    "\n",
    "<p>This is a paragraph!</p>\n",
    "\n",
    "<h2>This is a Subheading!</h2>\n",
    "\n",
    "<table>\n",
    "  This is a table!\n",
    "  <tr>\n",
    "    <th>The header in the first row.</th>\n",
    "    <th>Another header in the first row.</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>An entry in the second row.</td>\n",
    "    <td>Another entry in the second row.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<ul>\n",
    "  This is an unordered list!\n",
    "  <li>This is the first thing in the list!</li>\n",
    "  <li>This is the second thing in the list!</li>\n",
    "</ul>\n",
    "<div>Specifies a division of the document, generally with additional attributes specifying layout and behavior.</div>\n",
    "A <span>span is similar</span> but occurs in the middle of a line.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved the HTML document above as <a href=\"basic.html\">basic.html</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web vs Internet\n",
    "\n",
    "The web (or www or World Wide Web) is different from Internet in a couple ways.\n",
    "\n",
    "First the web is just part of the internet. The internet includes plenty of pieces unrelated to the web, like email and ssh, although the web has become a dominant piece.\n",
    "\n",
    "But in a deeper sense, the internet is a set of protocols used for transferring data, together with the infrastructure that run those protocols. The web is handled by one of those protocols, a high-level protocol called HTTP. HTTP express how requests are made by clients and how documents are returned by servers. So the web is really just a set of HTML documents sitting on HTTP servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Requests\n",
    "\n",
    "To get data from the web, you need to make a HTTP request.  The two most important request types are:\n",
    "\n",
    "* GET (queries data, no data is *sent*)\n",
    "* POST (updates data, *data must be sent*)\n",
    "\n",
    "Usually HTTP requests are sent by browsers (like Chrome or Safari) but `curl` is a command line program for sending HTTP requests.  It's easy to send a `GET` request to a url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://madrury.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`curl` can also send POST requests, but with a bit more effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -H \"Content-Type: application/json\" -H 'User-Agent: DataWrangling/1.1 matthew.drury@galvanize.com' -d '{\"action\": \"parse\", \"format\": \"json\", \"page\": \"Unicorn\"}' https://en.wikipedia.org/w/api.php  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to send this POST request in a much better way below, so don't worry about remembering how to do it with curl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping is the process of programmatically getting data from the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pipeline.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Load table into a data frame.\n",
    "\n",
    "Lets load the Super Metroid speedrun leaderboards at [Deer Tier](http://deertier.com/Leaderboard/AnyPercentRealTime) into a Mongo database, and then load this database into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# Requests sends and recieves HTTP requests.\n",
    "import requests\n",
    "\n",
    "# Beautiful Soup parses HTML documents in python.\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Check out the website in a browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to check out the website in a browser.\n",
    "\n",
    "Open the `Developer Tools` to get a useful display of the hypertext we will be working with.\n",
    "\n",
    "The table we will need is inside a `<div>` with `class=scoreTable`.  Looking closely the structure is like this:\n",
    "\n",
    "```\n",
    "<div class=scoreTable>\n",
    "  <table>\n",
    "    <tr>..</tr>\n",
    "    ...\n",
    "    <tr>...</tr>\n",
    "  </table>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Each row has a `title` attribute that contains some interesting data:\n",
    "\n",
    "```\n",
    "<tr title=\"Submitted by Oatsngoats on: 19/10/2016\">\n",
    "```\n",
    "\n",
    "Inside each row, the columns have the following data:\n",
    "\n",
    "```\n",
    "rank, player, time, video url, comment\n",
    "```\n",
    "\n",
    "This should be enough infomation for us to get to scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Send a GET request for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deer_tier_url = 'http://deertier.com/Leaderboard/AnyPercentRealTime'\n",
    "r = requests.get(deer_tier_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A status code of `200` means that everything went well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out the raw hypertext in the `content` attribute of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Save all the hypertext into mongo for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client.metroid\n",
    "pages = db.pages\n",
    "\n",
    "pages.insert_one({'html': r.content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Parse the hypertext with BeautifulSoup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the beautiful part of the soup.  Parsing the HTML into a python object is effortless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, \"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (soup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Navigate the data to pull out the table information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the structure of the table we are looking for:\n",
    "\n",
    "```\n",
    "<div class=scoreTable>\n",
    "  <table>\n",
    "    <tr>..</tr>\n",
    "    ...\n",
    "    <tr>...</tr>\n",
    "  </table>\n",
    "</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = soup.find(\"div\", {\"class\": \"scoreTable\"})\n",
    "table = div.find(\"table\")\n",
    "\n",
    "# This returns an iterator over the rows in the table.\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "# Let's store each row as a dictionary \n",
    "empty_row = {\n",
    "    \"rank\": None, \"player\": None, \"time\": None, \"comment\": None\n",
    "}\n",
    "\n",
    "# The first row contains header information, so we are skipping it.\n",
    "for row in rows[1:]:\n",
    "    new_row = copy.copy(empty_row)\n",
    "    # A list of all the entries in the row.\n",
    "    columns = row.find_all(\"td\")\n",
    "    new_row['rank'] = int(columns[0].text.strip())\n",
    "    new_row['player'] = columns[1].text.strip()\n",
    "    new_row['time'] = columns[2].text.strip()\n",
    "    new_row['comment'] = columns[4].text.strip()\n",
    "    all_rows.append(new_row)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(all_rows[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Load all the rows into a Mongo database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we collected all the rows into python dictionaries, this is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.metroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deer_tier = db.deer_tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in all_rows:\n",
    "    deer_tier.insert_one(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check from the command line that the data is really in there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Load all the rows into a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though there is no real reason to, let's load all the rows from the Mongo database just to give a more thorough example of how you can go about things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = deer_tier.find()\n",
    "super_metroid_times = pd.DataFrame(list(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_metroid_times.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_metroid_times = super_metroid_times.drop(\"_id\", axis=1)\n",
    "super_metroid_times = super_metroid_times.set_index(\"rank\")\n",
    "super_metroid_times.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal Achieved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Large-ish Exercise**: Scrape the leaderboads for [Ocarana of Time](http://zeldaspeedruns.com/leaderboards/oot/any) into a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Use a web API to scrape Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia provides a free API to programatically collect data.  This service is *designed* for programmers to interact with.\n",
    "\n",
    "[Wikipedia API Documentation](https://www.mediawiki.org/wiki/API:Main_page)\n",
    "\n",
    "A high level summary of the documentation:\n",
    "\n",
    "> Send a POST request to https://en.wikipedia.org/w/api.php with a JSON payload describing the data you want, and the format in which you want it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia wants us to identify ourselves before it will give us data.  The `User-Agent` section of a HTTP header contains this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'GalvanizeDataWrangling/1.1 matthew.drury@galvanize.com'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://en.wikipedia.org/w/api.php'\n",
    "\n",
    "# Parameters for the API request: We want the Unicorn page encoded as json.\n",
    "payload = {'action': 'parse', 'format': 'json', 'page': \"Unicorn\"}\n",
    "\n",
    "r = requests.post(api_url, data=payload, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.json().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a lot of data back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.json()['parse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Store the Data in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MongoDB modules\n",
    "from pymongo import MongoClient\n",
    "#from bson.objectid import ObjectId\n",
    "\n",
    "# connect to the hosted MongoDB instance\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db.wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not collection.find_one(r.json()['parse']):\n",
    "    collection.insert_one(r.json()['parse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicorn_article = collection.find_one({ \"title\" : \"Unicorn\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(unicorn_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (unicorn_article.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Retrieve and store every article (with associated metadata) within one link\n",
    "\n",
    "We want to hop from the 'Unicorn' article. *Do not follow external links, only linked Wikipedia articles*\n",
    "\n",
    "HINT: The Unicorn Law article should be located at: \n",
    "'http://en.wikipedia.org/w/api.php?action=parse&format=json&page=Unicorn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = unicorn_article['links']\n",
    "\n",
    "pprint.pprint(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's request each of these documents, and store the result in our collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "\n",
    "    payload = {'action': 'parse' ,'format': 'json', 'page' : link['*'] }\n",
    "    r = requests.post(api_url, data=payload, headers=headers)\n",
    "\n",
    "    # check to first see if the document is already in our database, if not, store it.\n",
    "    try:\n",
    "        j = r.json()\n",
    "        if not collection.find_one(j['parse']):\n",
    "            print(\"Writing The Article: {}\".format(j['parse']['title']))\n",
    "            collection.insert_one(j['parse'])\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Find all articles that mention 'Horn' or 'Horned' (case insensitive)\n",
    "\n",
    "* Use regular expressions in order to search the content of the articles for the terms Horn or Horned. \n",
    "* We only want articles that mention these terms in the displayed text however, so we must first remove all the unnecessary HTML tags and only keep what is in between the relevant tags. \n",
    "* Beautiful Soup makes this almost trivial. Explore the documentation to find how to do this effortlessly: http://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "* Test out your Regular Expressions before you run them over every document you have in your database: http://pythex.org/. Here is some useful documentation on regular expressions in Python: https://docs.python.org/3/howto/regex.html\n",
    "\n",
    "* Once you have identified the relevant articles, save them to a file for now, we do not need to persist them in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile our regular expression since we will use it many times\n",
    "regex = re.compile(' Horn | Horned ', re.IGNORECASE)\n",
    "\n",
    "with open('wiki_articles.txt', 'w') as out:\n",
    "\n",
    "    for doc in collection.find():\n",
    "        \n",
    "        # Extract the HTML from the document\n",
    "        html = doc['text']['*']\n",
    "\n",
    "        # Stringify the ID for serialization to our text file\n",
    "        doc['_id'] = str(doc['_id'])\n",
    "\n",
    "        # Create a Beautiful Soup object from the HTML\n",
    "        soup = BeautifulSoup(html)\n",
    "\n",
    "        # Extract all the relevant text of the web page: strips out tags and head/meta content\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # Perform a regex search with the expression we compiled earlier\n",
    "        match = regex.search(text)\n",
    "\n",
    "        # if our search returned an object (it matched the regex), write the document to our output file\n",
    "        if match:\n",
    "            try:\n",
    "                print(\"Writing Article: {}\".format(doc['title']))\n",
    "                json.dump(doc, out) \n",
    "                out.write('\\n')\n",
    "            except UnicodeEncodeError as e:\n",
    "                print(e)\n",
    "\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
