{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We will be implementing logistic regression using the gradient descent algorithm.\n",
    "\n",
    "#### References:\n",
    "* [Andrew Ng's Machine Learning Lecture Notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf)\n",
    "\n",
    "#### Notes on Implementing Gradient Descent:\n",
    "* Implementing gradient descent can lead to challenging debugging. Make sure you implement your algorithm using very simple pieces (functions or methods) which you then combine into the full algorithm.  This allows you to check the correctness of the individual functions, which makes mistakes easier to track down. You may also try computing values by hand for a really simple example (1 feature, 2 data points) and make sure that your methods are getting the same values.\n",
    "* Numpy is your friend. Use the power of it! There should only be one loop in\n",
    "your code (in `fit`). You should never have to loop over a numpy array. See the\n",
    "numpy [tutorial](https://docs.scipy.org/doc/numpy/user/quickstart.html) and\n",
    "[documentation](http://docs.scipy.org/doc/).\n",
    "\n",
    "## Basic\n",
    "### Part 1: Create Data\n",
    "\n",
    "1. Generate a dataset using sklearn's make_classification function.\n",
    "\n",
    "    ```python\n",
    "    X, y = make_classification(n_samples=100,\n",
    "                                n_features=2,\n",
    "                                n_informative=2,\n",
    "                                n_redundant=0,\n",
    "                                n_classes=2,\n",
    "                                random_state=0)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import src.logistic_regression_functions as lrf\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# set larger font sizes and the style\n",
    "np.set_printoptions(suppress=True)\n",
    "font_size = 24\n",
    "mpl.rcParams.update({'font.size': font_size})\n",
    "mpl.rcParams['xtick.labelsize'] = font_size-5\n",
    "mpl.rcParams['ytick.labelsize'] = font_size-5\n",
    "#plt.style.use('bmh')\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100,\n",
    "                             n_features=2,\n",
    "                             n_informative=2,\n",
    "                             n_redundant=0,\n",
    "                             n_classes=2,\n",
    "                             random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76605469,  0.18332468],\n",
       "       [-0.92038325, -0.0723168 ],\n",
       "       [-0.98658509, -0.28692   ],\n",
       "       [ 1.70910242, -1.10453952],\n",
       "       [ 1.9876467 ,  1.77624479],\n",
       "       [ 3.86274219,  2.63325914],\n",
       "       [-1.12836011, -0.42276158],\n",
       "       [-1.10074198, -2.56042975],\n",
       "       [-1.53716448,  1.10502647],\n",
       "       [-0.32372692,  0.55626974],\n",
       "       [-1.28532883, -1.30819171],\n",
       "       [ 3.35973253, -1.79506345],\n",
       "       [-1.27034986,  1.2678044 ],\n",
       "       [-0.71023363, -1.13058206],\n",
       "       [-1.12933108,  0.68766176],\n",
       "       [ 0.24820673, -0.50979271],\n",
       "       [-2.47263494, -0.48661246],\n",
       "       [-1.11573423,  1.43370121],\n",
       "       [-1.2141474 ,  1.97698901],\n",
       "       [-1.25860859, -0.18289652],\n",
       "       [-0.53583409,  1.10698637],\n",
       "       [ 0.92328145, -1.30813451],\n",
       "       [ 2.02751248, -0.4032571 ],\n",
       "       [ 2.1871814 ,  2.03196825],\n",
       "       [ 0.51281456,  0.43299453],\n",
       "       [-2.8118071 , -0.34553805],\n",
       "       [ 0.31338067,  1.12073484],\n",
       "       [ 0.27299061, -0.32110537],\n",
       "       [-1.18986266,  0.42434508],\n",
       "       [-0.11213297, -0.90719743],\n",
       "       [ 1.23567148,  1.5552506 ],\n",
       "       [-0.70329192, -0.6054158 ],\n",
       "       [ 1.3382818 , -0.98613257],\n",
       "       [-3.19826339, -1.25732069],\n",
       "       [ 0.76438953, -0.67959801],\n",
       "       [ 0.95252962, -0.79347019],\n",
       "       [-0.43793316, -1.24378126],\n",
       "       [ 2.40620516, -0.10017113],\n",
       "       [ 0.68152068,  0.39390608],\n",
       "       [ 0.45139447, -0.00247553],\n",
       "       [-1.45709006,  0.48668119],\n",
       "       [ 1.69989125, -1.66130052],\n",
       "       [-0.98035846, -1.40246886],\n",
       "       [-0.26822526,  0.0458931 ],\n",
       "       [-1.85016853, -3.58754622],\n",
       "       [-0.10789457,  1.34057624],\n",
       "       [ 1.27458364, -2.5215955 ],\n",
       "       [-0.44736931, -0.26805121],\n",
       "       [-0.00811816,  2.11564734],\n",
       "       [-2.22244349, -1.62073375],\n",
       "       [-0.89436588,  0.92995032],\n",
       "       [-0.2415271 ,  0.4559465 ],\n",
       "       [ 0.85041166, -0.2083118 ],\n",
       "       [-1.06938289,  0.41718036],\n",
       "       [-0.99510532,  1.23195055],\n",
       "       [-0.83183986,  0.7216695 ],\n",
       "       [-1.24743076,  0.70921659],\n",
       "       [ 1.22964368, -0.96154156],\n",
       "       [-0.90014214,  1.78037474],\n",
       "       [ 0.5710682 , -1.23267396],\n",
       "       [ 0.21654345, -0.07156026],\n",
       "       [-0.15902752, -2.38076394],\n",
       "       [-0.59732184, -1.20114435],\n",
       "       [ 1.04421447,  2.02899023],\n",
       "       [ 0.95508388, -1.62184212],\n",
       "       [-0.90334395,  1.79445113],\n",
       "       [-1.27395967, -0.04378433],\n",
       "       [ 0.25020227, -0.10486202],\n",
       "       [-0.89552621,  0.35117341],\n",
       "       [-1.17921312,  0.54481881],\n",
       "       [-1.20374176, -0.28978825],\n",
       "       [ 1.22006997, -0.82718247],\n",
       "       [ 2.34137626,  1.47049892],\n",
       "       [-0.61529301, -0.86400813],\n",
       "       [ 1.79574591,  1.87834887],\n",
       "       [-0.78091068,  1.85082392],\n",
       "       [-0.13650855,  0.73389996],\n",
       "       [ 1.73590335, -0.86953695],\n",
       "       [-0.6358122 ,  1.35329628],\n",
       "       [-0.68304936, -0.80022106],\n",
       "       [ 0.85144036,  2.42548085],\n",
       "       [-1.11745336,  1.26661394],\n",
       "       [ 0.67311496,  0.5593132 ],\n",
       "       [-1.14855777,  0.86582546],\n",
       "       [-1.45125944, -0.76288442],\n",
       "       [ 0.8566997 , -0.44358419],\n",
       "       [ 1.91002859, -2.20119016],\n",
       "       [-1.04999632,  1.83240861],\n",
       "       [-0.07818714, -0.35452887],\n",
       "       [ 0.52390262, -1.97242756],\n",
       "       [-1.36672011, -1.05286598],\n",
       "       [ 0.21726178, -0.47182417],\n",
       "       [ 1.35081491, -0.19464386],\n",
       "       [-1.41363563, -0.3599673 ],\n",
       "       [ 1.86752028, -0.20572167],\n",
       "       [-0.98944929,  0.37343616],\n",
       "       [-0.869385  , -0.86069962],\n",
       "       [ 2.53026908,  0.38025157],\n",
       "       [-0.46122013, -3.697436  ],\n",
       "       [-2.05321581,  0.42234144]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We use two features so that we can visualize our data. Make a scatterplot with the first feature on the horizontal axis and the second on the vertical axis. Depict the two categories using different colors or different symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,15))\n",
    "\n",
    "colors = ['blue' if y_i==1 else 'green' for y_i in y]\n",
    "\n",
    "ax.scatter(X[:, 0], X[:,1], c=colors);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Just by eye-balling, make an estimate for a decision boundary of the form `x_2 = m*x_1 + b`. (Note that we haven't fit a logistic regression to this yet, but if we did, the line of constant `p=0.5` has `m = -beta_1 / beta_2` and `b = -beta_0 / beta_2`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_2 = m*x_1 + b\n",
    "#m = 10\n",
    "\n",
    "#b = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cost function\n",
    "\n",
    "In order to be able to evaluate if our gradient descent algorithm is working\n",
    "correctly, we will need to be able to calculate the cost.\n",
    "\n",
    "Recall the log likelihood function for logistic regression. Our goal is to *maximize* this value.\n",
    "\n",
    "![Cost Function](images/likelihood.png)\n",
    "<!--\n",
    "\\ell(\\boldsymbol\\beta) = \\sum_{i=1}^{n} y_i \\log ( h(\\mathbf{x_i}) ) + (1-y_i) \\log (1 - h(\\mathbf{x_i}))\n",
    "-->\n",
    "\n",
    "Recall that the hypothesis function *h* is defined as follows:\n",
    "\n",
    "![hypothesis](images/hypothesis.png)\n",
    "<!--\n",
    "h(\\mathbf{x_i}) = \\frac{1}{1+e^{-\\boldsymbol\\beta\\mathbf{x_i}}}\n",
    "-->\n",
    "\n",
    "Since we will be implementing Gradient *Descent*, which *minimizes* a function, we'll look at the cost function below, which is just the negation of the log likelihood function above.\n",
    "\n",
    "![cost function](images/cost.png)\n",
    "<!--\n",
    "J(\\boldsymbol\\beta) = - \\sum_{i=1}^{n} y_i \\log ( h(\\mathbf{x_i}) ) + (1-y_i) \\log (1 - h(\\mathbf{x_i}))\n",
    "-->\n",
    "\n",
    "The gradient of the cost function is as follows:\n",
    "\n",
    "![gradient](images/gradient.png)\n",
    "<!--\n",
    "\\nabla J(\\boldsymbol\\beta) =\n",
    "\\left[\n",
    "\\frac\\partial{\\partial\\beta_1}J(\\boldsymbol\\beta),\n",
    "\\frac\\partial{\\partial\\beta_2}J(\\boldsymbol\\beta),\n",
    "\\ldots,\n",
    "\\frac\\partial{\\partial\\beta_p}J(\\boldsymbol\\beta)\n",
    "\\right]\n",
    "-->\n",
    "\n",
    "Each partial derivative will be computed as follows (for `j >= 1`):\n",
    "\n",
    "![partial](images/partial.png)\n",
    "<!--\n",
    "\\frac\\partial{\\partial\\beta_j}J(\\boldsymbol\\beta) =\n",
    "\\sum_{i=1}^n \\left( h(\\mathbf{x_i}) - y_i \\right ) x_{ij}\n",
    "-->\n",
    "\n",
    "1. To verify that your implementations are correct, compute the following _by hand_. Of course, you can use a calculator/google/wolfram alpha/python.\n",
    "\n",
    "\n",
    "    |                   | feature 1 | feature 2 |   y |\n",
    "    | ----------------- | --------: | --------: | --: |\n",
    "    | **x<sub>1</sub>** |         0 |         1 |   1 |\n",
    "    | **x<sub>2</sub>** |         2 |         2 |   0 |\n",
    "    | **x<sub>3</sub>** |         3 |         0 |   0 |  \n",
    "    \n",
    "\n",
    "\n",
    "    1. Using the data above, compute the **value of the cost function**. Initialize your coefficients: β<sub>1</sub> =1, β<sub>2</sub> =1. For now, assume β<sub>0</sub>=0 (that is, we are not considering an intercept term).\n",
    "    \n",
    "        Hint: you will use\n",
    "        (β<sub>1</sub>x<sub>1,1</sub> + β<sub>2</sub>x<sub>1,2</sub>)\n",
    "        while computing your hypothesis function for the first data point.\n",
    "    \n",
    "    2. Using the data above, compute the **gradient of the cost function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_1:0.7310585786300049, h_2:0.9820137900379085, h_3:0.9525741268224334\n",
      "4.8217499605431176 1.6950861587058217\n"
     ]
    }
   ],
   "source": [
    "def h(b1, b2, x1, x2):\n",
    "    value = 1 / (1 + np.exp(-(b1*x1 + b2*x2)))\n",
    "    #print(f'h({x1}, {x2}) with b1={b1}, b2={b2} is {value}')\n",
    "    return value\n",
    "\n",
    "\n",
    "partial_1 = (h(b1=1, b2=1, x1=0, x2=1) - 1) * 0 + (h(b1=1, b2=1, x1=2, x2=2) - 0) * 2 +(h(b1=1, b2=1, x1=3, x2=0) - 0) * 3\n",
    "\n",
    "partial_2 = (h(b1=1, b2=1, x1=0, x2=1) - 1) * 1 + (h(b1=1, b2=1, x1=2, x2=2) - 0) * 2 +(h(b1=1, b2=1, x1=3, x2=0) - 0) * 0\n",
    "\n",
    "h_1 = h(b1=1, b2=1, x1=0, x2=1)\n",
    "h_2 = h(b1=1, b2=1, x1=2, x2=2)\n",
    "h_3 = h(b1=1, b2=1, x1=3, x2=0)\n",
    "\n",
    "print(f'h_1:{h_1}, h_2:{h_2}, h_3:{h_3}')\n",
    "\n",
    "print(partial_1, partial_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In `logistic_regression_functions.py`, implement `predict_proba` and `predict` functions. `predict_proba` will calculate the result of `h(x)` for the given coefficients. This returns float values between 0 and 1, which should be interpreted as conditional probabilities. `predict` will round these values so that you get a prediction of either 0 or 1. An optional argument is provided for the threshold, which is defaulted to `0.5`.  **Note:** The names of these functions were chosen to align with `sklearn`'s conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted probability vector is [0.3775406687981454, 0.7310585786300049, 0.9525741268224334]\n",
      "The predicted class vector is [0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "X_sample = np.array([[0, 1], [2, 2], [3, 0]])\n",
    "y_sample = np.array([1, 0, 0])\n",
    "coeffs = np.array([1, -0.5])\n",
    "\n",
    "\n",
    "p = lrf.predict_proba(X_sample, coeffs)\n",
    "y_hat = lrf.predict(X_sample, coeffs)\n",
    "#cost = lrf.cost(X_sample, y_sample, coeffs)\n",
    "#grad = lrf.gradient(X_sample, y_sample, coeffs)\n",
    "\n",
    "print(\"The predicted probability vector is {}\".format(str(p)))\n",
    "print(\"The predicted class vector is {}\".format(str(y_hat)))\n",
    "# print(\"The cost function at these coefficients is {}\".format(str(cost)))\n",
    "# print(\"The gradient of the cost is {}\".format(str(grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In `logistic_regression_functions.py`, implement `cost` and `gradient`. You should be able to use the `predict_proba` function you implemented above. Make sure to check that you get the same values as you computed above.\n",
    "\n",
    "    In a terminal, you should be able to run your function like this:\n",
    "\n",
    "    ```python\n",
    "    import logistic_regression_functions as f\n",
    "    import numpy as np\n",
    "    X = np.array([[0, 1], [2, 2], [3, 0]])\n",
    "    y = np.array([1, 0, 0])\n",
    "    coeffs = np.array([1, 1])\n",
    "    f.cost(X, y, coeffs)\n",
    "    ```\n",
    "\n",
    "    Make sure to do `reload(f)` if you make changes to your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Implement Gradient Descent\n",
    "\n",
    "Now we are going to implement gradient descent, an algorithm for solving\n",
    "optimization problems.\n",
    "\n",
    "Below is psuedocode for the gradient descent algorithm. This is a generic\n",
    "algorithm that can solve many convex optimization problems (of which logistic regression is an example).\n",
    "In our case, the β we are solving for is the coefficient vector in logistic regression.\n",
    "\n",
    "In this pseudocode, and in our implementation, we will stop after a given number\n",
    "of iterations. A more common approach is to stop once the incremental\n",
    "improvement in the optimization function is sufficiently small, while still terminating after a fixed number of iterations so as not to create an infinite loop if things go badly during the optimization.\n",
    "\n",
    "    Gradient Descent:\n",
    "        input: J: differential function (optimization function)\n",
    "               α: learning rate\n",
    "               k: number of iterations\n",
    "        output: local maximum of optimization function J\n",
    "\n",
    "        initialize β (often as all 0's)\n",
    "        repeat for k iterations:\n",
    "            β <- β - α * gradient(J)\n",
    "\n",
    "You are going to be completing the code stub in `GradientDescent.py`.\n",
    "\n",
    "1. Start by taking a look at the starter code. Note how the `GradientDescent` object is initialized. It takes a cost function and a gradient function. We will pass it the functions that we wrote above. Here's example code of how we'll be able to run the Gradient Descent code once you've completed all the functions.\n",
    "\n",
    "    ```python\n",
    "    import logistic_regression_functions as f\n",
    "    from GradientDescent import GradientDescent\n",
    "\n",
    "    gd = GradientDescent(f.cost, f.gradient, f.predict)\n",
    "    gd.fit(X, y)\n",
    "    print(\"coeffs:\", gd.coeffs)\n",
    "    predictions = gd.predict(X)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement the `fit` method. Follow the pseudocode from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement the `predict` method. It should just call the `predict_func` function that was taken as a parameter.\n",
    "\n",
    "    This will be a kind of boring function. But later it will get more interesting if we do any preprocessing on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "\n",
    "### Part 4: Run gradient descent & compare\n",
    "\n",
    "Now we're ready to try out our gradient descent algorithm on some real data.\n",
    "\n",
    "1. Run your version of gradient descent on the fake data you created at the beginning.\n",
    "\n",
    "    **Note:** If you're having trouble getting it to converge, run it for just\n",
    "    a few iterations and print out the cost at each iteration. The value should\n",
    "    be going down. If it isn't, you might need to decrease your learning rate.\n",
    "    And of course check your implementation to make sure it's correct. You can\n",
    "    also try printing out the cost every 100 iterations if you want to run it\n",
    "    longer and not get an insane amount of printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Do you get a similar decision boundary to your eyeballed guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run sklearn's `LogisticRegression` on the fake data and see if you get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Add Intercept\n",
    "\n",
    "Ideally we would like to also have an intercept. In the one feature case, our equation\n",
    "should look like this: `y = mx + b` (not just `y = mx`). We solve this by adding\n",
    "a column of ones to our feature matrix.\n",
    "\n",
    "1. Implement `add_intercept` in\n",
    "`logistic_regression_functions.py` and use it to modify your feature matrix\n",
    "before running gradient descent.\n",
    "\n",
    "    ```python\n",
    "    def add_intercept(X):\n",
    "        \"\"\"Add an intercept column to a matrix X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: A two dimensional numpy array.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X: The original matrix X, but with a constant column of 1's appended.\n",
    "        \"\"\"\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Modify the `__init__` method of `GradientDescent` so that it can take a boolean parameter `fit_intercept`:\n",
    "\n",
    "    ```python\n",
    "    def __init__(self, cost, gradient, fit_intercept=True):\n",
    "        # code goes here\n",
    "    ```\n",
    "\n",
    "    If you set `fit_intercept` to be False, it should work the same way as before this modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Check that you get a similar decision boundary to the line you eyeballed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "### Part 6: Standardization\n",
    "\n",
    "If you try running your gradient descent code on some of the data from the [logistic regression](https://github.com/GalvanizeDataScience/logistic-regression) exercises, you'll probably have issues with it converging. You can try playing around with alpha, the learning rate (one option is to decrease the learning rate at every iteration).\n",
    "\n",
    "An easier way is to *standardize* the data: shift and scale the data so that the mean is 0 and the standard deviation\n",
    "is 1. To do this, we compute the mean and standard deviation for each feature\n",
    "in the data set and then update the feature matrix by subtracting the mean from each value and then dividing by the standard deviation.\n",
    "\n",
    "1. Commit your code! Run a `git commit -m \"some message\"` so that if you goof\n",
    "things up with your changes you don't lose your previous version.\n",
    "\n",
    "2. Add some methods to the `GradientDescent` class to calculate the scale factors and to scale the features (if the parameter is set.)\n",
    "    * **Note:** Make sure to scale before you add the intercept column. You don't want to try and scale a column of all ones.\n",
    "\n",
    "3. Modify the `__init__` method of the `GradientDescent` class so that it can take a boolean `standardize` parameter. Add calls of the above functions to the `fit` method if `standardize` is `True`.\n",
    "\n",
    "4. Make sure to standardize the features before you call the `cost` or `gradient` function.\n",
    "\n",
    "    * **Note:** You should calculate mu and sigma from the *training* data and use those values of mu and sigma to scale your test data.  Applying standardization to the entire data (before a train test split) is a subtle form of data leakage, and applying independent standardization to the test data can often result in overly optimistic measures of model performance.\n",
    "\n",
    "5. Run your code on the fake data and make sure you get the same results.\n",
    "\n",
    "6. Try running your code on the data from [logistic regression](https://github.com/GalvanizeDataScience/logistic-regression). Does it converge?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Regularization\n",
    "\n",
    "Recall that regularization helps us deal with overfitting. Let's implement L2 regularization (Ridge).\n",
    "\n",
    "We will be adding the following term to the cost function.\n",
    "\n",
    "![regularization](images/regularization.png)\n",
    "<!--\n",
    "\\lambda \\sum_{j=1}^p\\beta_j^2\n",
    "-->\n",
    "\n",
    "1. Again, don't forget to commit first!\n",
    "\n",
    "2. Modify your cost to include the above term. You should add an additional parameter to the cost function called `lam` which is the lambda in the regularization term.\n",
    "\n",
    "3. Modify your gradient function to include the gradient of the above term. You should add the `lam` term here as well.\n",
    "\n",
    "4. When you instantiate the `GradientDescent` object, you will need to use a new version of these two functions. Here's how you can create them:\n",
    "\n",
    "    ```python\n",
    "    def cost_regularized(X, y, coeffs):\n",
    "        return cost(X, y, coeffs, lam=1)\n",
    "    \n",
    "    def gradient_regularized(X, y, coeffs):\n",
    "        return gradient(X, y, coeffs, lam=1)\n",
    "    ```\n",
    "\n",
    "    Note that you won't actually need to modify your `GradientDescent` algorithm at all.\n",
    "\n",
    "\n",
    "***Always commit before you start a new part! If you muck up your previously working solution, you'll want to get back to it!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 8: Termination\n",
    "\n",
    "Instead of terminating after a fixed number of iterations, we can terminate when the incremental improvement in the cost function is sufficiently small.\n",
    "\n",
    "1. Add a parameter `step_size` to the `fit` function. Terminate the loop once the incremental decrease in the cost function is smaller than `step_size`. Note that this means you'll have to evaluate the cost function at each iteration and compare it to the previous value of the cost function. Specify your function to rely on the step_size parameter instead of the num_iterations parameter if the step_size\n",
    "is not None.\n",
    "\n",
    "2. Figure out what a good value for the `step_size`. If it's too large, you won't make it to the optimal solution. If it's too small, it will take too long to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9: Stochastic Gradient Descent\n",
    "\n",
    "*Stochastic Gradient Descent* is a variant of the gradient descent algorithm that in practice converges faster. The difference is that at each iteration, stochastic gradient descent only uses *one* training example for its update. Here is the pseudocode. `n` here is the number of training examples.\n",
    "\n",
    "```\n",
    "Randomly shuffle examples in the training set.\n",
    "Repeat until step size is sufficiently small:\n",
    "    for i = 1, 2, ... n:\n",
    "        β <- β - α * gradient for x_i\n",
    "```\n",
    "\n",
    "Here's the cost function for a single data point **x<sub>i</sub>**:\n",
    "\n",
    "![stochastic cost](images/stochastic_cost.png)\n",
    "<!--\n",
    "J_i(\\boldsymbol\\beta)} = \\log ( h(\\mathbf{x_i}) ) + (1-y_i) \\log (1 - h(\\mathbf{x_i})\n",
    "-->\n",
    "\n",
    "Here is the formula for the partial derivatives that make up the gradient:\n",
    "\n",
    "![stochastic partial](images/stochastic_partial.png)\n",
    "<!--\n",
    "\\frac{\\partial J_i(\\boldsymbol\\beta)}{\\partial \\beta_j} = \\left( h(\\mathbf{x_i}) - y_i \\right ) x_{ij}\n",
    "-->\n",
    "\n",
    "Note that we shuffle the training examples and then go through them in order so that we use every training example before repeating.\n",
    "\n",
    "1. Implement a `fit_stochastic` method that uses the stochastic gradient descent algorithm to fit the model.  You may have to implement alternate versions of the `cost` and `gradient` functions that accept different types for their arguments.\n",
    "\n",
    "2. Use ipython's `timeit` to compare the runtime of the standard gradient descent algorithm and stochastic gradient descent. Also compare their results to verify that they return the same optimal coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 10: Newton's Method for a single variable\n",
    "\n",
    "While gradient descent and stochastic gradient descent are two of the most common optimization techniques, they are not the only techniques used by data scientists. Newton's Method is a root-finding algorithm. When we apply Newton's Method to the derivative of a function, we can find the roots of the derivative, which is equivalent to finding the extrema of a function.\n",
    "\n",
    "1. Newton's Method can be used for finding the **roots of a differentiable function**. You may have learned this algorithm in your high school calculus class. Here is an overview of how the algorithm works.\n",
    "    ```\n",
    "    1. Provide an initial guess x0 for the location of the root.\n",
    "\n",
    "    2. Calculate f(x0)\n",
    "\n",
    "    3. If f(x0) is less than your pre-defined tolerance, then stop. Otherwise continue.\n",
    "\n",
    "    4. Find the x-intercept of the line tangent to f(x) at x0.\n",
    "\n",
    "    5. Update x0 := the x-intercept found in step 4.\n",
    "\n",
    "    6. Return to step 2.\n",
    "    ```\n",
    "    \n",
    "    Here is an [animation of how it works](https://en.wikipedia.org/wiki/Newton%27s_method#/media/File:NewtonIteration_Ani.gif)\n",
    "\n",
    "    Equivalently, this iterative method uses this update formula:\n",
    " \n",
    "    ![update formula](images/newtonupdate1.png)\n",
    "\n",
    "\n",
    "2. Newton's Method can also be used to find **local optima of any twice-differentiable function**, such as the cost function, by approximating the zeros of a function's derivative. \n",
    "\n",
    "    First, we can set \n",
    "\n",
    "    ![update formula](images/newtonupdate3.png)\n",
    "\n",
    "    Then, our new update formula will take the form \n",
    "\n",
    "    ![update formula](images/newtonupdate2.png)\n",
    "\n",
    "    And, we continue updating until the derivative, _g(x)_, is less than our tolerance level.\n",
    "\n",
    "    Note: if our function is quadratic, this method will find the vertex in one step.\n",
    "    \n",
    "\n",
    "3. **Exercise:**  Using Newton's Method for optimization, write a function that returns one of the local minimas of the quartic function f(x) = x<sup>4</sup> + 2x<sup>3</sup> - 5x<sup>2</sup> - 8. \n",
    "\n",
    "    ```python\n",
    "    def newton_quartic(x0, epsilon):\n",
    "    '''\n",
    "        Parameters\n",
    "\t----------\n",
    "        x0: Initial guess for x0.\n",
    "\tepsilon: Tolerance level for convergence.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "        optim: Cartesian coordinates of minima.\n",
    "    '''\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
